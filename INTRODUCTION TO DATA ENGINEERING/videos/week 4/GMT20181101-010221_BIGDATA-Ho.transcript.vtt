WEBVTT

1
00:00:03.060 --> 00:00:03.540
All right.

2
00:00:04.650 --> 00:00:06.600
Okay, so there was a thing.

3
00:00:07.740 --> 00:00:10.200
I'm gonna try. Yeah, that's

4
00:00:21.000 --> 00:00:21.270
And

5
00:00:23.700 --> 00:00:27.660
This is what I did last time right before the horrible noise began. So let's find out if that works.

6
00:00:29.550 --> 00:00:35.580
Sure. As far as why is sharing posts. I don't want to share because they want it, just to be working. Oh, there we go. What I get a gap.

7
00:00:41.130 --> 00:00:41.430
Sorry.

8
00:00:46.980 --> 00:00:57.630
No, I do want to quit. That's why I said quit. All right, great. Okay, so, but yeah. Alright, so anybody who shouldn't see on my screen. What do they see okay so

9
00:00:59.820 --> 00:01:03.960
Do you see my crappy handwriting. Oh, good. Okay, so

10
00:01:05.850 --> 00:01:12.930
In keeping with the fact that we we took some breaks to do things like with you guys read the news and play with various bits of software and do some live demos and stuff.

11
00:01:13.530 --> 00:01:27.690
As a result, our lecture schedules and not precisely stamped where the material live lecture and this nicely bracket events. We basically have shifted. So, because of this phase shift. We'll start off with some crappy doodling on by hand.

12
00:01:28.920 --> 00:01:30.570
To review things. So what did we do we've

13
00:01:31.680 --> 00:01:34.500
Last week and the week before we started looking at spark.

14
00:01:35.820 --> 00:01:43.470
And particularly, we looked at sparks principal data type, the RD. And we looked at transformations which when

15
00:01:47.940 --> 00:01:48.780
I do this.

16
00:01:49.980 --> 00:01:50.310
Okay.

17
00:02:02.370 --> 00:02:03.090
This is just

18
00:02:05.040 --> 00:02:06.480
This really sucks.

19
00:02:10.080 --> 00:02:11.880
Yeah. Classes trash.

20
00:02:14.040 --> 00:02:17.790
In this window can't be resized because evidently that like

21
00:02:20.430 --> 00:02:26.820
All right, screen. I'm not gonna say share this thing. I'm just going to talk equals just have to imagine in their minds what they are.

22
00:02:32.190 --> 00:02:34.410
And sharing is positive and

23
00:02:35.730 --> 00:02:42.030
Now, iTunes open because that makes sense. God, God take up. Yeah.

24
00:02:43.290 --> 00:02:45.030
Just stop my

25
00:02:48.240 --> 00:02:49.380
Share screen.

26
00:02:50.490 --> 00:02:52.110
Share the correct screen.

27
00:02:55.860 --> 00:02:58.620
All right, I have circumstantial evidence that's being shared.

28
00:03:00.420 --> 00:03:00.930
See

29
00:03:02.490 --> 00:03:02.760
What

30
00:03:04.740 --> 00:03:14.520
Okay, great. Alright. Alright, so, uh, yeah. So apologize. We're going to different rooms from the room are normally in because the first dream when we plug things in, we get these

31
00:03:14.880 --> 00:03:25.350
Louder Than bombs. Your shattering bursts of statics and the rooftop speakers, which were just glorious because the first time you hear when he literally jumped on your flesh, like you, you actually instinctively recover.

32
00:03:25.860 --> 00:03:31.050
And then some teacher was walking into the room and I was like, Oh, I should warn you that, and then

33
00:03:33.450 --> 00:03:43.470
I got just part of it to tell him I was wanting him to something, but I didn't actually get the subject of the warning out before it happened, watching his body shorter clenched up that's like the horrible blast of noise to send it was

34
00:03:44.550 --> 00:03:46.740
Probably very unpleasant. So anyway,

35
00:03:47.760 --> 00:03:50.340
We hope that this won't happen here in this new room across the hallway.

36
00:03:51.600 --> 00:03:58.920
I may lose consciousness partway through the lecture because like there are these bright fluorescent lights directly in my eyes and so I can almost see nothing here but

37
00:04:01.470 --> 00:04:05.190
If I turn off the ones that the France in the rooms return. It's Halloween.

38
00:04:06.240 --> 00:04:17.040
Is just gonna bother people. Okay, I'll just be up there doing like I should have brought a flashlight, I couldn't. I can't fire goes through, you know, the flashlight under your face. You know, I couldn't

39
00:04:19.170 --> 00:04:23.880
I'm trying, I'm trying, I'm bringing these much scarier than the candidate experience.

40
00:04:24.930 --> 00:04:28.350
But, you know, another be exhausted all our audio tricks. This is perhaps all we're left with

41
00:04:29.700 --> 00:04:37.320
Okay. So last week we talked about spark as I was fumbling through saying and what do we do, right, we

42
00:04:38.250 --> 00:04:45.990
We looked at our TVs, the principal sort of data type that underlie spark. We saw how they're sort of distributed Uncharted and partition across machines.

43
00:04:46.440 --> 00:04:55.140
We saw the spark notion of transformation which when applied to an RDP produces a new arty arty D from it and we looked at a bunch of examples of filter and map.

44
00:04:57.480 --> 00:04:58.170
flat map.

45
00:04:59.760 --> 00:05:07.800
A few others. And then we look at actions which caused a chain of transformations that produce our DVDs from earlier our DVDs to actually do some

46
00:05:08.100 --> 00:05:19.290
Things become reified and the computations implied by those transformations actually occur as a result is produced. And so we saw example, example for those like count and take take sample and probably a few others.

47
00:05:20.610 --> 00:05:34.830
And so we we noticed a couple things right. It was nicer than Hadoop and that you could like work at interactively in Scala shell and type cans and get results back relatively quickly, as opposed to writing you know vomit. This piles of clunky Java mapper and producer code.

48
00:05:36.030 --> 00:05:44.220
You know Scala. We'll talk more about that as we go on. But, you know, hopefully people are starting to get their scalawags firmed up a little bit, as they go on.

49
00:05:44.850 --> 00:05:54.360
But you know there were some subtle things right, you know, you're still basically dealing with kind of poorly structured data, it can be easy. If you're breaking line into pieces around comma separated values or something to

50
00:05:54.900 --> 00:06:00.990
Rip ask for the fifth column. And when you really wanted the sixth column or forget that there's a girl based or stuff like that and it's a little bit fragile.

51
00:06:01.320 --> 00:06:14.160
And so the main topic of what will be the core material of this week's lecture after we finished up from last week will be spark SQL and different data sets which aim to solve those problems in ways that might be familiar if you've used our

52
00:06:15.270 --> 00:06:20.790
Various Python type systems. But before we go on to that will finish talking about the RTB stuff from last week, so

53
00:06:22.110 --> 00:06:22.830
Last week,

54
00:06:25.920 --> 00:06:35.790
Last week. Ah, good, good. Okay, last week we just started talking about partitioning shuffling. So I'll jump back to the beginning of the partitioning shuffling

55
00:06:37.230 --> 00:06:38.400
Material sorry there's some

56
00:06:39.660 --> 00:06:41.100
water spots on the screen.

57
00:06:45.690 --> 00:06:53.640
Okay, you can't see what I see in your broccoli lucky. Okay, so back to the partitioning. So when we had an already view, which is sort of

58
00:06:54.300 --> 00:07:03.960
An attraction for some large piece of data broker. In this case, these lines of data, the left side in that that particular grayish block called data dot txt and we see there's 1000 lines of whatever

59
00:07:04.440 --> 00:07:12.450
And then the representation is Rd Rd breaks it into partitions. In this case, five logical partitions, each containing about one fifth of the

60
00:07:13.110 --> 00:07:20.100
entries from that original piece of data on the left and then if we look on the right, these red boxes represent nodes in our spark cluster.

61
00:07:20.430 --> 00:07:29.730
Which are becoming the homes where those logical partitions actually physically fit. So, you know, you might have said, you know, creating Rd from some file and HDFS

62
00:07:30.240 --> 00:07:39.000
The spark runtime would have gone to our HDFS read that file suitably broken the lines are entries in that file scattered across the nodes in your cluster. And then they on the right.

63
00:07:39.270 --> 00:07:49.440
You know, roughly, in this case, there's five partitions your thousand entries are broken into chunks that you have about 200 each will see more later about just how that breaking process happens and what actually occurs.

64
00:07:50.850 --> 00:07:56.010
Partitions were like, not only to the way the data is actually stored physically across the cluster or memory.

65
00:07:56.460 --> 00:08:06.510
But also to how computationally flows right because just like in Hadoop MapReduce spark aims to basically move the computation that's going to be performed close to where the data is going to operate on lives.

66
00:08:07.020 --> 00:08:14.190
Do the computation there locally. In this case, your memory, rather than by streaming it in HDFS and then do something so

67
00:08:14.550 --> 00:08:17.700
What ends up happening is if you've got stages of computation mistakes you've got

68
00:08:18.090 --> 00:08:26.790
These three stages 01 and two represented by the blue boxes at the left, each of them have some number of tasks in them to in the first to the second or the third

69
00:08:27.600 --> 00:08:38.280
The Cluster Manager will take these tasks and has to find some way to schedule them on executive executives, which basically live on the notes of the spark cluster the executor in this case is there sort of spark equivalent of what the

70
00:08:38.670 --> 00:08:47.040
The task trackers in Hadoop MapReduce for remember that you're the HDFS side of the house. There is a data node that that maintain the metadata and coordinated the actions.

71
00:08:47.310 --> 00:08:54.000
Of sorry a name node which maintain the metadata and coordinate the actions of so called Data notes that were actually in charge of holding and providing the data.

72
00:08:54.330 --> 00:09:00.330
And similarly, there was a job hacker tracker which babysat tasks crackers correctly performing MapReduce jobs.

73
00:09:00.750 --> 00:09:10.950
Similarly here there's spark Cluster Manager will take these stages but has to them and try to schedule them to run on executors in some way that respects the logical data flow dependencies that exists between them.

74
00:09:12.870 --> 00:09:18.960
We talked a little bit last week about how the number of partitions is chosen saw a couple of options. We won't talk about this too much now.

75
00:09:19.560 --> 00:09:26.670
But we'll skip forward, we saw that you could control the partition size and loading data, you know, you could ask how many politicians have had

76
00:09:27.000 --> 00:09:34.800
You can tell it to report fishing with a different number of partitions and you could to increase it to say so in this example here we see getting ready to text file from the

77
00:09:35.340 --> 00:09:44.310
local file system. It came in with two partitions. In this example, because we probably running out of the local laptop machine that had two cores and spark was just set up to, you know, break things up that way.

78
00:09:44.610 --> 00:09:54.480
And then he says every partition into 1000 partitions, which, you know, in that environment probably insane but it's just done for illustration purposes, and then he saw that the resized

79
00:09:55.350 --> 00:10:06.450
RDS Rd that came back called file which is now a new article resized with the number of partitions. You can also go the other direction coalesced basically taken already spread over a large number of partitions to a smaller number partitions.

80
00:10:07.110 --> 00:10:20.160
And some people work in the homework. So I've already sort of stumbled across this in the context of when I actually have an odd result. And I want to save the thing, how do I, you know, have it not get saved, and as many pieces as it might get Satan, and we'll talk more about that later.

81
00:10:22.230 --> 00:10:32.340
Now remember every transformation on an odd is lazy and nothing really happens until you basically perform some action downstream. So I could have an odd that came from reading a thing from the file.

82
00:10:32.610 --> 00:10:43.410
The file system, you know, say SC exe file some path. And then when I do that nothing's really happening. Except I've created this logical entity called Nardi t which is kept track of where it's going to draw a different, but it doesn't actually written

83
00:10:44.700 --> 00:10:56.460
And this is why sometimes if you get that finally wrong refer to something that doesn't exist, you don't notice it until 10 steps later when you try to do account or some other action on it and then blows up immediately with a file not found because it tried to reunify

84
00:10:56.760 --> 00:11:00.930
The sequence of our DVDs and it failed right at the outset, but it couldn't actually build the first stage.

85
00:11:02.550 --> 00:11:03.300
So,

86
00:11:04.500 --> 00:11:10.020
Obviously, when you have these things, the data might not all be in the places where you sort of need it to be. When repercussions.

87
00:11:10.230 --> 00:11:19.200
If you read partitioning, you know, and you have suddenly you say you had a partition that was all. You know, there was only one partition and you had a six node cluster and you can send three partition of six partitions.

88
00:11:19.740 --> 00:11:26.400
You know, it makes sense for five, six of that data to move somewhere else in the cluster. Right. So the repercussion operation CLI some motion.

89
00:11:27.540 --> 00:11:33.510
And that motion might be expensive, right, because when you actually do some action requires move. It's going to have to actually be moving between nodes.

90
00:11:33.840 --> 00:11:44.790
And we'll see ways of amortizing these costs, both with respect to manually partitioning, as well as some complications, things like reduced by key, which naturally caused the shuffling to occur. So

91
00:11:45.630 --> 00:11:53.820
Before we move into that new stuff. Any questions on the spark stuff to date, or this quick recap of the stuff that we ended last week partway through on

92
00:11:55.380 --> 00:12:01.470
Going once, going twice. I'll go a little bit slower so online. People have the chance to unmute themselves and

93
00:12:04.770 --> 00:12:07.890
Complaining or not. Okay. So, shuffling

94
00:12:09.060 --> 00:12:12.270
When does it happen. So here's the example.

95
00:12:13.740 --> 00:12:21.360
Of, you know, what are the dependencies. So imagine at the very left side of this chain. We've got the driver, which is talking to say our spark shell and our spark session.

96
00:12:21.570 --> 00:12:26.160
And we started life with some in memory Scotland collection to say we made a range one to a million.

97
00:12:26.610 --> 00:12:35.070
You know created some array of strings or whatever it was. And then we called paralyze on spark context. And then that thing got produced converted into an Rd

98
00:12:35.700 --> 00:12:38.970
In this case it's a subclass have already been called parallel collection Rd

99
00:12:39.390 --> 00:12:49.890
And there it is, right, in this case, it looks like it has five partitions. You see that left most stack of my boxes and so whatever our local stuff was that we push through paralyzed and spark context got

100
00:12:50.460 --> 00:12:54.990
partitioned into these five chunks are stripes or partitions and scattered across the cluster.

101
00:12:55.650 --> 00:13:10.290
Now, the next transformation here that happens to that first started. He called this start ed on the left is a map. And when you think about how math works. What is what is map do who wants to call it but but map actually does when faced with a Rd full of things.

102
00:13:13.410 --> 00:13:22.320
It was shot shy or doesn't remember. I know some people remember because they're emailing me questions, but no one wants to speak. No one will speak.

103
00:13:22.980 --> 00:13:29.880
No one in the room. Okay, fine. We'll just don't notice. Okay. So, Matt. Basically when applied would already be takes a function

104
00:13:30.390 --> 00:13:39.420
And that function consumes an element of whatever they already have useful. So imagine your Rd is an RDP of string or an Rd event, the function would take into a string, respectively.

105
00:13:39.750 --> 00:13:46.680
And it converts it into something, right. So, you know, a simple example of one of these functions. If I hadn't have List of spin already do strings.

106
00:13:46.950 --> 00:13:57.510
Would be a function that takes a string s and calls to link on it. Right. And so now Rd of string has turned into already have it where all the strings that just been replaced by whatever their, their likes our needs.

107
00:13:58.500 --> 00:14:01.110
But the essence is that this is basically a one to one.

108
00:14:01.890 --> 00:14:05.160
Transformation. Right. You know, if you haven't already to head analytics.

109
00:14:05.370 --> 00:14:19.920
Map is going to take each one of those elements and vomit out some new element that came from that element right so if you had an already have any strings in our example and you mapped you know X goes to start to string on to those strings, you would know haven't already have an S.

110
00:14:21.090 --> 00:14:25.680
So, you know, there's no need for anything to shuffle the book there right you know the partitions. You had five of them.

111
00:14:25.740 --> 00:14:34.350
Each one of them contain some of them are things. The new already there's going to be fired partitions. Each one will have the same number of things added before they're just going to be different things, because they'll be images.

112
00:14:34.560 --> 00:14:40.140
Of whatever in the we're in the previous Rd under the function. You've mapped over. Does that make sense to everybody. Okay.

113
00:14:41.880 --> 00:14:52.410
So now we haven't started the call pairs. So I mentioned that just took everything in the first RD. And, you know, put it into two bowls and stuck a one hour or something kind of like we were kind of samples.

114
00:14:53.250 --> 00:15:01.440
That's probably why they call that example pics. So we've got this already. Same number partitions, same number of items partition. Therefore, same number of items completely ignored.

115
00:15:02.100 --> 00:15:12.930
But now will be reduced by key. And so remember what reduced by keyword right if we had a pair that was taken interpreted as a key and a value that's each of the elements and that to to build this key value.

116
00:15:13.410 --> 00:15:22.770
Well, then we have to group everything associated with a single key in one place, along with a list of whatever values came along with them and you know those could be

117
00:15:23.160 --> 00:15:32.130
You know, they could come from two places right you know say we say our keys were words and the number one right in that case like you know the word

118
00:15:32.520 --> 00:15:45.690
Could be a hearing over and over again, any of those partitions of that already. Right. And so for every time I saw the comma one appear as a pair and that pair of RDS we have to send it across. After the reduced by key into the interview shuffle dark

119
00:15:46.230 --> 00:15:52.590
And then we get, you know, the key one, the Kiva, and then a list of however many one's got put together so

120
00:15:52.920 --> 00:15:56.700
The crisscross the lines in the very middle of this picture are meant to show that right

121
00:15:56.970 --> 00:16:05.880
That when you do a reduced by key. You know, we're going to pick places to put the keys, you know, in the target of partitions of the new RPG that comes about as a result of produced by key.

122
00:16:06.090 --> 00:16:11.880
But then we actually have to move the data with the values that were associated with those key. So they'll pile up in the right place. Is that, is that clear to everybody.

123
00:16:12.750 --> 00:16:24.390
Okay, so that's an example of sort of one of these, these shuffle dependencies, sometimes people use the term narrow dependency or wide dependency on and I don't really like those terms because they're not very clear.

124
00:16:25.320 --> 00:16:37.110
I think what what's intended by hero with the example that first map where things are kind of going nicely straight across and right you know it's narrow in the sense that everything kind of goes to a natural place that's kind of next to it.

125
00:16:37.650 --> 00:16:43.740
Whereas wide means stuff might come from all kinds of places on the left, Defense Commission and scattered all over right

126
00:16:44.610 --> 00:16:51.780
So sometimes maybe tall, you know, the way this picture is drawn tall might make more sense than wide, but neither the make much sense. Right. You know,

127
00:16:52.290 --> 00:16:55.080
I think it's a lot clearer to go with what this this diagram says

128
00:16:55.680 --> 00:17:04.740
That creates natural one to independence. I eat everything in the original Rd goes to one new thing. And we do our duty. So it's nicely one to one or rejected or

129
00:17:05.010 --> 00:17:10.500
One to one and on to or a one to one correspondence or whichever the synonyms for that concept, you want to use.

130
00:17:11.220 --> 00:17:20.640
Whereas the shuffle. Isn't that at all. Right. You know, it may not be one to one. All right, a whole bunch of things associated with a single key may have gone to the same place and that missing shuffle in the middle, right.

131
00:17:21.270 --> 00:17:25.530
Or, you know, you may have things that didn't come from anywhere because they just didn't happen to be key score. Right.

132
00:17:26.580 --> 00:17:28.980
And then the map on this thing again. And for some reason.

133
00:17:29.520 --> 00:17:37.500
And so, Mac, as we said earlier, is another one of these sort of went to London slashing arrow dependencies. And then we get some final RD. And then on the right, we do the

134
00:17:37.800 --> 00:17:45.930
Action which forces all these events to actually current even though you know all the definitions sort of logically said, what's going to happen when the actions performed so

135
00:17:46.470 --> 00:17:48.810
Narrow dependencies versus wide dependencies.

136
00:17:49.170 --> 00:18:00.540
Better term probably is one to one versus not one to one or and the things that are not one to one day off shuffling like this right there. They're going to scatter pass things around in some ways, although there are ways you can actually define how you want to move

137
00:18:02.340 --> 00:18:14.040
Most of the actions and the resulting shuffles right because it actually will a lot of the actions are collapsing stuff down, right. So, for example, count, right, or you know group by key or something actually

138
00:18:14.640 --> 00:18:21.900
Actually, you know, you can see in cases where a lot of these things. Basically the actions or an event that's actually the concentrate things and narrow them down.

139
00:18:22.770 --> 00:18:30.390
Most transformations don't really need to reduce need to produce shuffles because a lot of them are these direct things like map or flat Napper or you know even filter.

140
00:18:30.840 --> 00:18:35.220
Right, you think about what filter happens right it's not quite one to one anymore because something's just disappeared.

141
00:18:35.790 --> 00:18:46.320
Right. But on the other hand, the ones that survived did just kind of go straight across. So filter would be like one of these maps where some of those lines are just missing, right, because there's no need for something move on to the next stage that

142
00:18:47.760 --> 00:18:48.210
Map.

143
00:18:49.620 --> 00:18:54.420
So each element of identity. It might be part industry.

144
00:18:55.260 --> 00:19:04.260
Know RD. Is it is it parameters type. So you can have already have int, you can have already the string, you can have already have some other object entirely here on devising

145
00:19:04.980 --> 00:19:19.800
You can have already used to pairs you think if you didn't know where to kind of example, one of the first things you can take an Rd of words and map them two pairs of work, comma, it so we're on can be cooked by key with the teams. The word and then be reduced by adding ones right

146
00:19:21.960 --> 00:19:36.060
Mike was a busy week when I was on a city CSV file. Yeah, so I'm getting the IV. Yeah. And then when I'm getting anybody not free. Yep. He knows what value it is

147
00:19:36.660 --> 00:19:42.450
Giving ministry. Right. And that's because the RD started life is already a string.

148
00:19:42.840 --> 00:19:55.080
So when you did that you probably just split on string and comment. Yes. So remember what the split method on Java screens do and Scala strings mostly wrapped Java streams and a few extra things, but most of the underlying behaviors, the JavaScript.

149
00:19:55.950 --> 00:19:58.680
Java string split method takes a string.

150
00:19:59.220 --> 00:20:01.110
And it will you call it on the stream.

151
00:20:01.410 --> 00:20:05.880
You tell it what you want to exclude around it finds all the places where that thing you're splitting around are

152
00:20:06.030 --> 00:20:12.210
And then it gives you back and array of all the stuff on the side of things. So just fooling around the comments. The comments vanish, we get back a collection of

153
00:20:12.450 --> 00:20:20.040
All the stuff that was separated by commas. So, in your case, because you took a string and split it around some character, you're just going to get an array of strings back right

154
00:20:20.340 --> 00:20:25.650
So that's why when you get those things. So you've had like line goes to mind God split comma and then what the status.

155
00:20:25.890 --> 00:20:34.440
Or something right that's what was happening live dot split comma was giving you back an array of strings array of strings and then five was taking the one position five

156
00:20:35.910 --> 00:20:36.150
Year.

157
00:20:39.060 --> 00:20:39.870
Plan.

158
00:20:42.270 --> 00:20:46.590
Yeah, that's, that's what I see the text file does right basically leads to the file as

159
00:20:47.820 --> 00:20:57.750
A lot like that's all it knows that's all it knows about it. Right. All it knows is about. All it really knows is that it's a file of text and it's going to figure out where lines in based on where the new lines are

160
00:21:00.060 --> 00:21:00.510
Going to

161
00:21:01.830 --> 00:21:05.910
You when we, when you start talking about different data sets will see there are better tools for doing this.

162
00:21:06.180 --> 00:21:14.160
Because you'll notice that probably already example collected in class and probably already in the homework, you've had to do things like drop the first row because you know it's the column headers. Right.

163
00:21:14.370 --> 00:21:25.050
And you know, you can imagine cases where you probably want to keep those names around you. So you have to store them in your back pocket or something. You'll see it with their friends and data sets. There's actually nicer ways where you can have the UK guys take care of that for you.

164
00:21:26.190 --> 00:21:30.570
Which which is certainly nice because like that get saving what was column seven again. I don't remember.

165
00:21:31.290 --> 00:21:39.300
That's that's messy and it also you write a lot awkward stuff right you know you know line starts split comma five you know.to end

166
00:21:39.570 --> 00:21:45.180
Right, because it's a strange, right, it's, it's the lines of string that you split around commerce to get a collection of strings.

167
00:21:45.360 --> 00:21:52.800
Then you took one of the strings out but you ultimately want to do some arithmetic. So you have to coerce it into a number, right. That's, that's, that's, you know,

168
00:21:53.610 --> 00:21:56.850
All this all these objects have data types, right, and

169
00:21:57.270 --> 00:22:05.640
They're not really being used in that case right you're having to mentally keep track of what they are. So you can physically coerce them into what you what you need them to be at the right point

170
00:22:06.090 --> 00:22:15.840
If you get it wrong. Right. See, line five isn't. It's actually a string you try to call on it. Well, it's just going to throw an exception because it's an illegal argument for, you know, to it right

171
00:22:16.530 --> 00:22:29.790
And so having that kind of structure and type information or or scheming and going, what kind of raised and then the way somebody guys raise it, you know, if you have it in can use it. You want it like it's going to protect you from a lot of the sort of errors and mishaps.

172
00:22:31.230 --> 00:22:31.620
So,

173
00:22:33.030 --> 00:22:42.570
Partitioning and truffling various actions and transformations will result in the need to take things either directly to a corresponding place to Jason where you came from the new Rd

174
00:22:43.020 --> 00:22:47.940
Or they'll require these things to be kind of shuffled and chopped up and hashtag swiveled across multiple things.

175
00:22:48.840 --> 00:22:54.240
And obviously the shuffling ones can be more expensive, right, because there's there's more overhead. There's more coordination, you know,

176
00:22:54.510 --> 00:23:04.680
The mapping is relatively easy and that you basically know I have this range of things. I'm going to take some or all of them. And I'm just going to shuffle them wholesale to one place, but in the other case, it's like I got my nice and some here and some there.

177
00:23:05.190 --> 00:23:06.780
And you know accomplish even more expensive.

178
00:23:08.460 --> 00:23:11.460
So sometimes you actually want to shuffle.

179
00:23:11.730 --> 00:23:19.740
Right, the Preakness shuffle. We saw this is the thing that just kind of occurred naturally as part of the reduced by key operation right you know we had a bunch of things that were key value pairs.

180
00:23:19.980 --> 00:23:24.240
Reduced by T basically means put them all together sorted by T and then apply reduced to them.

181
00:23:25.980 --> 00:23:33.390
You know, shuffling. It's just a natural side effect of natural consequence of getting that done, but you know sometimes you want to shovel. So say for example, we had a

182
00:23:34.110 --> 00:23:42.750
An RGB on the left there that came from the state of that text file and then we did a filter right you know we got rid of numbers. We didn't like according to some

183
00:23:43.590 --> 00:23:54.090
filter function that takes some number and returns true if we like it false if you know. And so you see what happens here is the party. She's got really on uniform right apparently in the numbers less than or equal to 10

184
00:23:54.600 --> 00:24:02.100
We only like one in three whereas from 11 to 17 like everything and then you know 18 to 24 we got sort of fickle again.

185
00:24:03.240 --> 00:24:15.240
25 to 31 he became outright exclusionary only that 31 survive. And then, you know, our standards are falling by partition five and lead through all the way to five, eight support or something.

186
00:24:15.660 --> 00:24:20.130
But you can see we've got know what these politicians that are like really uneven a monkey and sizes. Right.

187
00:24:20.460 --> 00:24:27.630
And so we might wonder if it really makes sense to be storing these things in this way. If we've got like one partition on one node. It's only containing 2% of the data.

188
00:24:27.930 --> 00:24:40.410
Right, and then to these others are containing probably, you know, three quarters of it or more. And then there's a couple more containing single digit percentages. So we can actually force it. But really partition, you know, and actually take it and push it down to the number one.

189
00:24:41.730 --> 00:24:53.850
We only do this if we kind of observed this happening, right, or we knew something about our data or the operation, who performing where we were expecting to happen right and you know you could imagine there are cases where you get some some efficiency on this right because

190
00:24:55.110 --> 00:25:00.720
Remember, every separate partition requires for both you know transformations or actions potentially

191
00:25:01.020 --> 00:25:05.880
more work to be done right and if you let a whole bunch of these fiddly little partitions have almost nothing in them.

192
00:25:06.120 --> 00:25:14.190
Now you're getting the overhead of the Court of coordinating work where there's no way to work to be done right. So it's like you're sort of incurring unprofitable overhead on these things.

193
00:25:15.330 --> 00:25:28.380
So, you know, you'll notice this stuff when you look at the the UI or the log or the run output and you'll see things like some nodes might take much longer than others, right when you do a reducer map right and if you've got like eight nodes or

194
00:25:28.710 --> 00:25:37.260
It goes whether eight partitions and already and you see that six of them returned in roughly as long as it took to parse the query. And then the other two took like all of the time.

195
00:25:37.590 --> 00:25:46.950
You know, the natural question is, why you know what's special about those other two and one things you might notice is that, you know, hey, a couple of these partitions moment just kind of being an empty because an earlier stage produce something like this.

196
00:25:47.790 --> 00:25:59.820
So, you know, as you look at sort of optimizing your queries are optimizing programs or reacting to the to the counter counter for real will will spot stuff like this. And, you know, remember that this tool is this and you can use it sort of compensate for this.

197
00:26:00.660 --> 00:26:08.670
You probably don't want to do it up front. It'll be a premature opposite optimization and a lot of cases, right, because you may not actually know what's going to happen until it's already happened once.

198
00:26:09.390 --> 00:26:17.220
But you know when you spot the smell of is happening, like I've got a bunch of partitions and two of them take all the time and the rest of them basically don't take any time other than just the overhead.

199
00:26:17.550 --> 00:26:25.440
Then the natural thing to SPECT it's, there's nothing going on over here and we'll come back later to looking at how you can get this kind of information from the UI and tools.

200
00:26:27.540 --> 00:26:35.430
So we talked a bit about shuffling and how it determines where the partitions and various bits of the data and the analytics.

201
00:26:35.820 --> 00:26:39.240
Of it also ends up determining the boundaries of compete states.

202
00:26:39.450 --> 00:26:47.940
And we saw a little bit of this when we ran hive and we ran some really MapReduce jobs you to produce that DAG of, like, I got this map and then I gotta do this following reducing sometimes I

203
00:26:48.180 --> 00:26:52.350
Map and then reducing the other mapping another produce and we start out with already ease. It's like

204
00:26:52.620 --> 00:26:56.670
Okay, well I can do these maps. And then I can do this filter. But then I want to do a join

205
00:26:56.910 --> 00:27:04.110
But I can only do the join until I have finished all the work for producing anything I'm joining right so if you had one already that came from read some stuff.

206
00:27:04.350 --> 00:27:15.300
Map Filter, filter map again. And then another thing that was reading some other stuff map it filter it and then I want to join them obviously I can't do the joy until I did both of those sort of ancestral streams of work right.

207
00:27:16.110 --> 00:27:23.190
And that's what you see here, right, we got some complications that stage zero where we somehow creating already from that file one in the upper left.

208
00:27:23.430 --> 00:27:32.640
That are the one that the two partitions. We have stage to where you know we've got another stage where we're creating an Rd with these partitions P three MP4 in the lower left from file to

209
00:27:33.060 --> 00:27:42.360
We could run stage. Stage one at the same time. Right. You know, they don't have independent right each one of them is just going to storage and know rehydrating stuffing summer theme.

210
00:27:42.930 --> 00:27:49.020
But suppose stage to involves reduced by tea or a group by key or something, right.

211
00:27:49.320 --> 00:27:56.850
Then some of the keys that it wants to work with my come out of the RD on the top left, and somebody come to the RD on the lower left lower back.

212
00:27:57.210 --> 00:28:04.470
Right. And obviously stage to can be completed until I until both of those other ones are completed, right, or stage to enjoy.

213
00:28:04.770 --> 00:28:13.410
Right, like the example I just touched down. In which case, you know, you realize that stage too obvious and can't start until stage zero in stage one or both done it themselves at least a month.

214
00:28:13.710 --> 00:28:22.980
And then everything within Stage one is pretty this these kind of straight one to one transformations that problem maps or filters or something like that. And then you're going to do a forward right so

215
00:28:23.640 --> 00:28:30.450
The shuffling that you asked for, or in some cases the shuffling that's required by a thing like a reduced by key or a group by key or something like that.

216
00:28:30.720 --> 00:28:37.890
It will influence how the computer parts of your jobs are broken into stages, as well as influencing how the data is broken into partitions for storage.

217
00:28:38.370 --> 00:28:44.160
And so a stage basically is work that doesn't have to cross nodes to get done.

218
00:28:44.640 --> 00:28:55.740
Right. So if it's stuff. I can do actually without having to communicate across knows that it's a stage right and then once I have to communicate across by maybe gated on prior stages being done before I'm ready to actually proceed. Right, so

219
00:28:56.040 --> 00:29:04.710
Again, this example here at the stage zero and one, which could run in parallel, because they don't depend on one another and then stage to can even get started, until the has everything it needs from stages or one

220
00:29:06.780 --> 00:29:09.150
So the takeaway on that right

221
00:29:10.170 --> 00:29:11.940
sparkle partition your data on

222
00:29:12.840 --> 00:29:19.740
How that data is partition does you transform our TVs and converted. These may change, right, because some of the transformations. You do might do shuffles

223
00:29:19.920 --> 00:29:29.670
Or you might explicitly underneath and say, hey, I want to read partition this or coalesce this right. And so how do you, how do you use a tense right and so the answer is start simple.

224
00:29:30.540 --> 00:29:39.510
observe what's going on. No, your data. If you have some preliminary data explanation that tells you what the stage always produces really lumpy partitions where most of them are empty.

225
00:29:40.380 --> 00:29:48.030
Just because some quirk about the data is structured or are generated, then you might know that, hey, this is a good time, Nicole. So this is a good time to be partition down

226
00:29:49.110 --> 00:29:54.210
But, you know, you kind of have to sometimes see it happened the first time to sort of do the suspicion that work is

227
00:29:54.810 --> 00:30:04.590
And then you'll see as we move into spark SQL, it'll do some nice things for you automatically. But you know the times that this kind of manual or conscious attention to partitioning really matter.

228
00:30:05.070 --> 00:30:08.340
Typically are in cases where there's like not uniform a decent input data.

229
00:30:08.910 --> 00:30:17.070
Series not uniformity isn't like the computation you're applying you know you say you're filtering is getting rid of stuff really get an unfair and even way across the different topics.

230
00:30:17.370 --> 00:30:22.170
Or your data is heavy tailed in some strange way that you know you got a lot of stuff that sort of concentrated

231
00:30:24.180 --> 00:30:28.260
Let's see, that's probably all I need to say on him. Any questions so far.

232
00:30:31.710 --> 00:30:35.550
Nobody. Okay. And I know online people asking questions. Okay.

233
00:30:37.020 --> 00:30:38.820
Let's see. Alright, so

234
00:30:40.740 --> 00:30:46.920
There was a couple tricks here and actually a couple people have asked me questions about about things they were doing in the homework I'm

235
00:30:47.370 --> 00:30:53.040
Michael blue eyes seems on Michael. Are you online tonight. This is the piece that I said to watch out for tonight because

236
00:30:53.490 --> 00:30:57.060
It might impact ways, you would think about your, your reg ex problem.

237
00:30:57.720 --> 00:31:03.900
Not directly for the purposes of that assignment, but it's a trick where if you were going to do that. If I do you think you want to keep them on so

238
00:31:04.560 --> 00:31:13.710
Think about but map ducks right so map. Take some function and it applies to every element in every partition that makes up an odd.

239
00:31:14.100 --> 00:31:20.370
Right, so the examples we saw earlier, we're pretty simple right you know I was co mapping, like, you know, and what like as an example.

240
00:31:20.760 --> 00:31:30.870
Multiply by two is example or stuff like that, you know, that's pretty straightforward. Or, you know, a mapping that basically turned a string to string comma one pair for the word half example.

241
00:31:31.980 --> 00:31:43.410
But stay need to do more. Right. And so this first example is that, so we've got this airports already that we make by taking our airports comma separated value file and and turn them into ID to spark context. And then we have this thing where

242
00:31:44.010 --> 00:31:55.590
We're going to have a map function that's got expensive in his name so triggered advert truth and advertising is expensive. So we created some of it, see the new expensive connection. Maybe it's a connection to some sequel database, right.

243
00:31:56.160 --> 00:32:03.720
Where we want to do something. So we're going to, we're going to map onto the RD. And for every airport record in the RD. We're going to go perform a database query similar

244
00:32:03.990 --> 00:32:10.080
You know, maybe we want to look at who's the who's the administrator that that database and it's at that airport and it's just in some of the thing.

245
00:32:10.410 --> 00:32:16.050
Right, well, creating a database connections expensive, right. It's like multiple round trips to the database server, you have to, you know,

246
00:32:17.040 --> 00:32:22.500
Contacted in the first place. You probably have to send it a user ID in some credentials to authenticate yourself with it and

247
00:32:22.830 --> 00:32:28.500
There's probably multiple back and forth can just a handshake have connected, right. It's okay. I want to use your database. The database that

248
00:32:28.770 --> 00:32:32.190
We have for you. You're like, I'm this guy with this password that he says

249
00:32:32.490 --> 00:32:40.860
No, you're not go away and then you come back and try and connect and I'm this guy, this password movies like okay you got it right this time, you know, here's your, your session token, or whatever.

250
00:32:41.850 --> 00:32:51.180
You know that's expensive right you know that's a handful of network round trips say each one is like 10 milliseconds and say like sending a collection of takes 50 milliseconds, you know,

251
00:32:51.780 --> 00:32:57.840
That's 15 times 10 to the minus three seconds. I'm now going to apply it to an RGB with, you know, 100 million rows.

252
00:32:58.320 --> 00:33:09.600
Right, so that's 50 times 10 to the minus three times 100 that's 5000 and you know 10 to the minus three is five. So it's 5000 times five. So that's five times 10 to the eight

253
00:33:10.380 --> 00:33:17.580
That's, that's a lot of time, right, because basically everything you're doing in the map, you're going to try to reopen a database connection to do it right.

254
00:33:18.330 --> 00:33:23.850
But, you know, you still can imagine applications where you want to do that. Right. You know, I want to basically go to the database and consulted as an apple right

255
00:33:24.150 --> 00:33:35.490
So there's an alternative here called map partitions, where basically you have a map function as well. But you work at different and when it gets fed is different. So you'll notice up above.

256
00:33:36.210 --> 00:33:45.300
The map function that we just use with regular map. It takes as argument, whatever your details in this case, the importance of streaming. We do the same number.

257
00:33:46.020 --> 00:33:57.780
With net partitions what spark provides into the map function is an iterator over those things right so spark rather than the map function being called once per se and whole Rd on

258
00:33:58.170 --> 00:34:13.380
each of the nodes. This thing will get called one time, right, and then the argument will be something that the contents of iterate over right so in the top one that will get called once for each each entry in the RD. And that'll happen on each of the nodes that contains

259
00:34:14.730 --> 00:34:21.690
On the bottom one, it'll just happen once on that note, right. So in this case we create our new expensive connection.

260
00:34:22.620 --> 00:34:30.990
And then we've got this collection of things. The airport data things and then we're going to call it regular over this thing using a call to the expensive.

261
00:34:31.500 --> 00:34:38.820
Connection as its as its own right. So we haven't called this expensive create connection thing on each and every entry right basically called one time.

262
00:34:39.150 --> 00:34:45.150
And then the way spark and books. So basically feeds us in a collection that we can iterate over and then repeatedly reuse that expensive connection.

263
00:34:45.840 --> 00:34:56.430
Right, presumably, we're still talking over a network when we do it, but presumably that operations cheaper than, you know, setting the whole connection up was in the first place. And even if it was the same. At least you're cutting work half

264
00:34:57.030 --> 00:35:03.750
half times. How many times does that make sense. There is some really good feeling that we are getting

265
00:35:06.330 --> 00:35:06.960
Almost. Yeah.

266
00:35:07.980 --> 00:35:10.770
connection pooling, typically you have an extensive underlying the motivations.

267
00:35:11.130 --> 00:35:18.930
Right, you've got this resource that's expensive to create. So you're going to create when you're going to keep it around and with connection pooling you'll YOU'LL SCARE THEM AND MULTIPLE queries or multiple clients.

268
00:35:19.350 --> 00:35:26.550
In here, that's basically what we're doing is we've got this connection, we're going to create it once and then rather than calling this whole function, which has created the production inside it.

269
00:35:26.790 --> 00:35:35.160
We're going to call this other function which passes in the list of the things we're going to Babylon. Right. And then we can work on it using that thing. We only created that one time on the first entry.

270
00:35:35.760 --> 00:35:48.720
But the motivations to take an expensive resource that might have non trivial latency associated with its creation and reuse did a whole bunch of times. So here it looks like the regular Matt. Matt, I'm going to

271
00:35:49.950 --> 00:35:53.430
Be fast. What are the components are you

272
00:35:54.450 --> 00:36:01.530
Looking at an exponential rate become an article. Right. But that's why entry from the right, getting the better function.

273
00:36:05.550 --> 00:36:13.710
So inter inter rater is a spell it is it is it. This is basically like the iterator in Java, right, we're basically. But what is, what is an iterator job right, if

274
00:36:14.010 --> 00:36:20.160
You can, you know, if a collection in Java implements interval that means you can ask it for an iterative

275
00:36:20.730 --> 00:36:27.420
Right. And when you ask a collection that implements iterator for an interval. It gives you back a reference to an interface called iterator.

276
00:36:27.720 --> 00:36:38.460
Which allows you to do a small number of things you can ask the iterator. Hey, do you have more stuff we look at, and it can tell you yes or no. If it says no, you can just discard it and stop move on through, like if it says yes. Then you can say, okay, give me the next

277
00:36:38.970 --> 00:36:54.660
Right, and that's what you're being passed in this case right spark will create a simulator for you and pass it into the section for you. And then when you actually run over the generator. And so this down here, this isn't the spark RDP math. This is the map just the map on a regular iterator.

278
00:36:56.280 --> 00:37:01.170
Right but monument different do that together. Second, not math, right, the better food.

279
00:37:02.610 --> 00:37:05.490
Better than that function yet exceeds

280
00:37:06.600 --> 00:37:10.740
Expensive connection, not in and reach for data underscore right

281
00:37:12.630 --> 00:37:25.230
And then you say dot Eventbrite right and what would happen here as you come in, you create the expense of connection you use it one time it gets garbage collected at the end. And then you come back to you and you create a new one. Use it one time garbage collector repeat. I think I'm

282
00:37:26.340 --> 00:37:26.730
Taking

283
00:37:29.550 --> 00:37:31.890
It like that will get to see what it is.

284
00:37:33.120 --> 00:37:48.360
Yeah, and. Okay, so, so, so here's what happens. Right. Um, you know, with better map function sparkles you and rather than giving you already. It gives you a thing. You can iterate over that will basically allow you to pass over the elements, right. So really value.

285
00:37:53.580 --> 00:38:04.800
Know, so this is exactly this is the very same call here. Oh, the underscores that was throwing the one is underscoring the loss function over here. The second yeah yeah yeahs expects me to rather than

286
00:38:05.970 --> 00:38:13.410
An order. That's right. You know, something, something that you're what and something. What do pass the map partitions. Is it just the map function.

287
00:38:13.710 --> 00:38:26.850
It's one of these things that expecting iterator on the underlying type that in there already. But how does, how does the manner that function. Know that this time it was great. It was actually a lot

288
00:38:28.020 --> 00:38:42.840
I mean it's it's right, it's right there. It's like section right there in the scholar types, right up there. You've got just a string and here you've got an iterator and strings. So we see what do you do with these math was described my on

289
00:38:44.880 --> 00:38:51.000
Just to see if it works on exactly the same RD. And what's happening here is that instead of calling map you're calling Matt partitions.

290
00:38:51.360 --> 00:39:03.930
And the contract that what you pass in as the map function is different. Right, rather than being a thing that just eats the element type of the RD. It has to keep generators of that and then spark spark is responsible for actually creating the iterator and giving it to you.

291
00:39:06.090 --> 00:39:10.440
That's right. Yeah, because you don't actually, you know, you don't actually call the

292
00:39:10.860 --> 00:39:19.470
Map function right when you say hey spark Mac this function, you know, that means it's on spark to deal with that I wrote a function that you gave it and it's the same in both cases.

293
00:39:19.980 --> 00:39:24.630
This underscore here in case people were thrown by this and you didn't get through all of this scholar or cheap.

294
00:39:25.680 --> 00:39:30.840
In a lot of cases in Scotland where there's only one possible thing a thing could be, you can just use underscores the place

295
00:39:31.290 --> 00:39:43.170
So basically, in this case, you know, airport Mac is a function that only includes one thing. So we could we could have said this was airport dot map. A goes to expensive connection enrich airport data have a

296
00:39:43.560 --> 00:39:51.090
Right. Because, but because there's just one W parameter there. We can just write an underscore here and not named right because there's no ambiguity in that case.

297
00:39:52.050 --> 00:39:58.650
It's a little tricky install it, because there's like 13 different things underscores mean depending on where they are, but one of them is this kind of, you know,

298
00:39:59.250 --> 00:40:02.250
It's it's a dummy single parameter thing. So like

299
00:40:02.820 --> 00:40:14.580
You know, if I was doing the other junk, we talked about earlier. So say I had a hierarchy of strings. And I wanted to convert it to an Rd instead represented their legs like I could do our TV dot MAC address goes to string dot

300
00:40:15.150 --> 00:40:24.420
Playing right. I can also do Rd map underscore, dot length, right, because you know that thing only appears once there's only one dummy parameter there, why even bother.

301
00:40:25.290 --> 00:40:32.430
Right. So when you see these like naked floating in limbo by themselves underscores and scholar that's that's that's what this context, it doesn't make sense.

302
00:40:33.510 --> 00:40:42.300
So short answer on this was, if you've got a map function that needs to rely on doing an expensive combination or creating an expensive resource.

303
00:40:42.870 --> 00:40:54.360
And you can use the same one over all the things you're mapping against then consider using map partitions with the suitably different signature for the function you give Matt partitions and then only doing that expensive thing one time or

304
00:40:55.170 --> 00:41:07.680
Another example was a regular expression match right when you create a regular expressions like you write the regular expression is a string of pluses and stars various types of brackets and other symbols and so on. But when you actually do the match.

305
00:41:08.550 --> 00:41:17.700
The regular expression library has to compile that regular expression into a state machine. Right. And in that state he actually gets used to the match, right. So if you are going to use the

306
00:41:18.240 --> 00:41:23.700
Regular expression to say, you know, you know, check for phone numbers, right, or find all the phone numbers all the lines of

307
00:41:23.940 --> 00:41:33.630
Phone numbers right so you can line goes to line you know matches the phone number, whatever. But if you had to create this regular expression for every line of the 10 million line file right creating the regular expression.

308
00:41:34.200 --> 00:41:45.450
It's not really expensive. So that would be another case where you want to use net partition know create the regular question once and then in the thing use it against the integrated or two to check the things you did not

309
00:41:50.040 --> 00:41:50.640
See here.

310
00:41:51.660 --> 00:41:55.680
Actually, that might be a typo on the slide I'm forgetting.

311
00:41:59.730 --> 00:42:04.320
But let's come back to that one. And maybe pretty pricey example. When you look at the documentation of see what the actual

312
00:42:06.750 --> 00:42:11.190
The actual signature that is now works up. This is something that's fairly rarely done because

313
00:42:11.880 --> 00:42:22.320
You know, most of the time you stay with entirely within spark. But there are cases where you might want to either create something expensive or rely on. I turned on an external thing where the context, you're talking to. It is expensive to create

314
00:42:23.280 --> 00:42:30.210
Same story for for each. I think this is one of the transformations we talked about last week there was a there was for each right so Matt.

315
00:42:30.750 --> 00:42:34.110
Basically goes over the partition and the map function.

316
00:42:34.680 --> 00:42:41.550
Full really already and the map function takes an element of whatever type isn't already and does something to it returns a result

317
00:42:41.880 --> 00:42:50.340
There's also a for each which can take a function. It doesn't return anything the equivalent of like a body function in Java or a function in Scala returns a unit.

318
00:42:51.300 --> 00:42:56.940
Unit is scholars name for the bottom of the type system, it's kind of, you couldn't avoid it doesn't have about um

319
00:42:57.750 --> 00:43:02.370
So when you do this, you're typically doing something for your side effects. Right. So say I wanted to

320
00:43:02.970 --> 00:43:17.430
I had a bunch of strings. I wanted to print them to the terminal or something. I could say, you know, collection dot for each print underscore and then it would just iterate over them and print all into the terminal. But, you know, print doesn't return anything right you know much

321
00:43:18.660 --> 00:43:20.370
Most well printing that context doesn't

322
00:43:20.910 --> 00:43:29.280
If you've got a function doesn't return anything can't really use it as a map because it doesn't have the right hand side. Right. So for each is basically it's semantically like that. But the thing

323
00:43:29.580 --> 00:43:32.310
The thing that you're mapping doesn't return anything so it doesn't return

324
00:43:33.060 --> 00:43:39.630
Entirely for side effects. So in this case, you've got, you can't go here where the expensive functions getting DB database connection.

325
00:43:39.900 --> 00:43:48.090
And then called database protection and safety airport. We just got right and so the first one is obviously bad because we're essentially creating this thing every time.

326
00:43:48.450 --> 00:43:54.420
You know, we're creating a whole day to face connection to do one piece of work and then abandoning it to the garbage collector and creating new one.

327
00:43:54.930 --> 00:44:00.510
similar story here, create the database connection. Once the map of honey before each partition.

328
00:44:00.960 --> 00:44:10.200
Spark calls it getting you an integrator of the things that it's expected me to operate on and then you could basically for each over that director and do the big question. And then there's your underscore again, for example.

329
00:44:11.460 --> 00:44:17.730
And so, you know, you can sort of you'll see these coming when you see them right if you know you're creating a thing that has

330
00:44:18.030 --> 00:44:27.750
No cost database connections are obvious right to go to your point about. That's why you have to exit polls and that's why you have technical motions everywhere the regular expression was a little bit less obvious. In fact, um,

331
00:44:29.310 --> 00:44:30.930
Does anybody know the book effective Java.

332
00:44:31.740 --> 00:44:41.580
Is it mixing a book. Yeah, so I think items six and the third edition. That book is avoid unnecessary object creation and he has an example where basically there's a reg ex match, it's done in the big tape loop.

333
00:44:41.880 --> 00:44:44.760
And it turns out to be incredibly expensive because basically what's happening.

334
00:44:45.030 --> 00:44:51.600
Is the reg ex every time you enter the loop, it's re compiling the reg ex producing that finite state machine that's used for the matching

335
00:44:51.840 --> 00:44:56.040
Using it once and then falling back through the loop again where we get we recreate anyone

336
00:44:56.430 --> 00:45:03.510
Right. And the example with 1000 times faster or something because you know once you have the to the state machine compiled

337
00:45:03.870 --> 00:45:11.580
Then it's going to run in time proportional to the length of the thing that it's tested I possibly even less time if if it's easily to feel tired or not really

338
00:45:12.420 --> 00:45:20.610
But, you know, you have to re compile this thing from scratch, then it's just a fixed non trivial over in the UK on every past. So it says it's item 60 the third edition and effective Java.

339
00:45:21.990 --> 00:45:23.580
Not trying to sell Josh's book.

340
00:45:25.920 --> 00:45:29.280
It's a good book to everyone. If you have to deal with Java, you should definitely have that book.

341
00:45:31.440 --> 00:45:34.200
So listening. Okay, so

342
00:45:36.630 --> 00:45:41.040
Let's see, where we going next. Okay. So short answer.

343
00:45:42.240 --> 00:45:46.980
Matt partition and for each partition or versions of the map and for each transformations

344
00:45:47.280 --> 00:45:59.760
That will allow you to amortize the cost of an expensive operation so that it doesn't have to happen once per element mapped it happens once per call to either Matt partition for each partition and then you're

345
00:46:00.150 --> 00:46:06.570
Provided function has to actually iterate over the results that spark provides to it up. One last thing from partitioning.

346
00:46:07.230 --> 00:46:14.070
You can actually choose your own rules for partitioning right um the default. One is a hash based on where basically computer hash based

347
00:46:14.460 --> 00:46:23.460
It computer hashtag for these things and then runs it through a thing. And then, you know, takes the remainder modular, the number of available partitions and has based either cluster size or how you configure it or whatever.

348
00:46:23.820 --> 00:46:30.330
And spread them around that way. But, you know, every hash function has an adversarial class inputs, right, you know,

349
00:46:30.870 --> 00:46:37.200
Seeing the same thing, a whole bunch of times would be one example. But, you know, if someone knows your hash function or just get unlucky.

350
00:46:37.410 --> 00:46:45.300
You can end up with a hash function, the dumps everything very uneven piles and on the same the same small number of spots. So in those cases you can provide your own

351
00:46:46.140 --> 00:46:56.670
Partition by extending the interface org Apache Spark partition of and it basically what you provide is the thing that given a key returns some kind of them for it.

352
00:46:57.090 --> 00:47:06.540
Right, so you can make this as clever, clever as you want. You can make it return little time which case, all of your stuff will get partitioned into partition one and all the other partitions ignored.

353
00:47:06.840 --> 00:47:17.940
Probably that's not going to solve any problem that you actually had but you know if you didn't realize that your data had pathologies that caused it to interact, that the existing hash based partner, you could provide your own here in order to get around it.

354
00:47:19.470 --> 00:47:24.120
This example here is obviously completely stupid because it takes the key and it tries to convert it to hint.

355
00:47:24.360 --> 00:47:36.510
And obviously nothing, anything that doesn't obviously have a way to convert to it or respond well to the essence of the thing is going to blow up horribly you know it's it's basically trying to cast a type two type that it can't possibly be cast to

356
00:47:37.620 --> 00:47:41.850
But, you know, again, this is a thing to consider when you know something about your data that you know

357
00:47:42.090 --> 00:47:51.960
It's not going to partition well in the default way. And usually the way you find that out and he just saw it happen bad and you saw it happen badly. More and more, more than once, and then you looked at a little bit and realized why it's happening.

358
00:47:53.730 --> 00:47:55.260
My after you

359
00:47:56.430 --> 00:47:58.290
You see a few

360
00:48:01.350 --> 00:48:16.320
Oh. That actually isn't on the slide. But if you look at the interface for partition or the documentation explains how to use it is pretty rarely used by my kids, it's helpful to know it exists because when the cases where you do need it. You absolutely because the alternatives are awful.

361
00:48:18.690 --> 00:48:21.180
And the end equals what is that

362
00:48:24.150 --> 00:48:29.970
Kind of thing but it uses equals four. I'll top my head. I don't remember what it uses the equals well. Oh, I think I probably know

363
00:48:32.460 --> 00:48:37.080
Yeah, I'm not gonna say it a lot because I wouldn't bet that it's true, but we can always come back to that. If someone's really fretful about it.

364
00:48:38.430 --> 00:48:39.120
Yeah.

365
00:48:44.340 --> 00:48:47.940
Effective Java by Josh Block below CH.

366
00:48:49.080 --> 00:48:55.080
Basically, if you have to work with Java any non trivial amount or maintainer read Java code written by other people.

367
00:48:55.800 --> 00:49:04.080
Or maintain or write Java code that other people will have to read it sort of a book that every Java programmer should have on the bookshelf. It's not like an introduction to the

368
00:49:04.500 --> 00:49:10.140
Language. It assumes that you already know it's somewhat, but it's a lot of good sort of practices that have evolved over the last 20 years

369
00:49:10.530 --> 00:49:20.130
Josh is one of the authors of the original a bit big hunks of the original Java class libraries. So a lot of the original collections libraries, he wrote, as well as things like begins during the decimals.

370
00:49:24.720 --> 00:49:32.940
The third edition just came out to the end of last year, beginning of this year and it's it's it was first published around 2001 or so. So

371
00:49:33.330 --> 00:49:40.380
The first edition Java had been in use for enough years that is starting to become clear what the best practices work and then the language has grown and evolved since then.

372
00:49:41.400 --> 00:49:49.680
And gain new features that needed new additions to the book and also you know what people believe we're getting bad ideas a little bit over time, but I think that

373
00:49:50.190 --> 00:49:58.230
Of the contents. The third edition. Probably the 70 or 80% and it's probably stuff that is strongly held over or helped over with minor changes from earlier versions.

374
00:49:58.590 --> 00:50:04.740
It's not like like oh my god, everything is wrong. You know how many years ago. It's now there's new things to be potentially not wrong about

375
00:50:07.410 --> 00:50:10.830
Let's see. Okay. So persistence in cash, so

376
00:50:12.690 --> 00:50:21.330
Remember that Rd transformations are lazy. Right, so I can make an already with no se dot txt file and nothing happens right all the thing doesn't say here's

377
00:50:21.630 --> 00:50:32.010
The that when it's called upon to do something will need to go to this text file defines data and then I could call map on that already. And I could call filter on the next already an app on the next filter for each

378
00:50:33.210 --> 00:50:39.780
Reduced by t and I can have this long chain of things but no real work has happened until an action actually happens on the end. Right.

379
00:50:40.890 --> 00:50:48.840
So, you know, the spark does these operations on stuff in memory, which, you know, certainly faster than reading it to and from disk and every stage.

380
00:50:49.680 --> 00:51:01.680
But sometimes you might actually want a result like an intermediate results they can play for us again or use a bunch of times. Right. And so you have a couple of ways to do that. Right. So in this example, we create an

381
00:51:03.300 --> 00:51:04.620
RDS strings from our

382
00:51:05.790 --> 00:51:14.370
Textbook War and Peace text file. We then filter the RTB by taking everyone in the RTB here we're calling the next interested clarity.

383
00:51:15.180 --> 00:51:23.100
But we say x we converted uppercase to sort of canonical eyes all of them. So capital war and lowercase, more and more people eating capital and

384
00:51:23.520 --> 00:51:28.590
You know, maybe it's needs to be considered W and Dr capitalized, that the ad is still the case.

385
00:51:29.250 --> 00:51:36.600
Or is it the shares own post or something, whatever, you know, you'll get all the capitalization happens. So we just cannot applies with uppercase and then you see if the line native world war.

386
00:51:37.080 --> 00:51:48.240
Right, and then we can basically save the RDS TXT file, which takes a path. And so it will go to slash slash 2.3 hyphen one because I didn't specify a file system type on the front of that.

387
00:51:48.810 --> 00:51:50.490
It'll default I think HDFS

388
00:51:50.850 --> 00:51:56.310
So you put that you take slash templates probably going to be the slash template your local file system. I think that's not what it defaults to

389
00:51:56.490 --> 00:52:01.590
So if you ever do one of those. You don't remember the file colon slash slash or the HDFS colon slash slash

390
00:52:01.800 --> 00:52:12.210
And you run it and you can't find the file anywhere goal thing in HDFS at the place that you think you put it, because I think most of these API's to actually default to using HDFS verbs, which is

391
00:52:13.380 --> 00:52:18.120
Somewhat natural but it's it's surprising. The first time because you think I thought I stayed and it's nowhere.

392
00:52:19.830 --> 00:52:26.880
And so the next thing here is we could another id by mapping canonicalization get uppercase and replace war with peace because I guess retired.

393
00:52:27.870 --> 00:52:36.750
Unending carnage and then we say that so you know that's neat, but what happened here is that we had to read the thing twice do

394
00:52:37.290 --> 00:52:49.890
You say this text file is an action. Right. So remember the transformations are lazy. They don't do it. So we started with one Rd that came from text file transformation, then we did a filter on it then you receive a text file.

395
00:52:50.850 --> 00:52:56.460
And that was the end of life for that already right filtered is just sitting around there. And then for math we went back to the original art.

396
00:52:56.940 --> 00:53:06.750
We did another app and we deliver status text file, which means that that first stage, before we did the map or in the second case for the method. In the second case, that whole reading of the file that happened again.

397
00:53:07.410 --> 00:53:17.160
Right. And if that file was really big that might be actually most of the cost of this right, it might have taken as much time to read the file is it good to just, you know, look at every line and do such trivial amount of work.

398
00:53:17.880 --> 00:53:30.900
So the natural thing you want to do is, you know, maybe, you know, if we know we have this chain of our DVDs to descend from one another. Maybe roll back to some point, like, you know, stick a pin in and say, Just don't be compute back before this point. And so this is how you do that.

399
00:53:32.400 --> 00:53:37.830
So if you do that first. That first transformation so called spark context text file, it'll say

400
00:53:38.370 --> 00:53:46.860
I'm interested in this file on disk. If someone eventually does it transformations and then performs the action. I'll go read the thing and then apply the transformations to it and then give them themselves.

401
00:53:47.280 --> 00:53:55.890
I can call r&d persist on this and that will keep the RD around rather than forcing the laziness to go all the way back to the beating the crap to do

402
00:53:56.250 --> 00:54:03.960
Right. So now when I do Rd filter to make filtered and already mapped to be mapped on the second two lines the reading of the file doesn't happen.

403
00:54:04.140 --> 00:54:11.670
Right. It only gets read the one time because they did the RTB persists there. And then when all these things have to walk back through the stages from the terminal action.

404
00:54:11.850 --> 00:54:18.300
Through transformations we get back up to the first one that was persisted and then we don't need to go any further because the results of that one has been kept around for us.

405
00:54:20.610 --> 00:54:21.120
Make sense

406
00:54:22.800 --> 00:54:23.460
To upper

407
00:54:27.720 --> 00:54:28.650
enlighten

408
00:54:30.900 --> 00:54:33.060
Me to do right so

409
00:54:34.680 --> 00:54:45.810
Yeah. And in fact, you are right that first Rd RD. It never changes after right so we create a new already called filtered by doing the filter on it, in which case we looked at the things containing more

410
00:54:46.890 --> 00:54:55.290
And in that case, we only kept lines that mentioned war. The second thing we did something different. We pacified all the lines that contain more by changing more to peace. Right, so you

411
00:54:57.870 --> 00:55:07.380
Know, uh, well I mean we we are doing an uppercase twice, but we're producing different results. Right. And, you know, the notion that resulted. I've seen by more than one thing.

412
00:55:07.800 --> 00:55:14.040
Right. The already called Rd that very first one is the text file already. These are immutable. They never changed once they're created

413
00:55:14.490 --> 00:55:29.460
So in the line where we do now filter equals Rd shelter, we're taking the already called Rd we're applying filter to it. We're creating a new our DD conceals right which is going to throw out some of the lines that don't contain more say, Yeah, you did a reading

414
00:55:31.410 --> 00:55:45.480
On that say can be like, yeah, you, you, you could imagine you could imagine next steps. Yeah, you could it be so you could do Val tact equals already be dot map x goes to x to every case and you could have persisted. At that point, even if you wanted to be.

415
00:55:47.190 --> 00:55:47.730
An action.

416
00:55:51.210 --> 00:55:55.920
Well persist basically means that after it's evaluated the one time it'll, it'll keep the r&d around right

417
00:55:56.160 --> 00:56:03.150
After it's been instantiate the one time, right. So basically, your cases. Imagine to application is super expensive and

418
00:56:03.420 --> 00:56:13.710
You wanted both of these things, they will take advantage of doing that work once then your suggestion is right, we created intermediate Rd called state Val capitalized equals Rd Matt X goes

419
00:56:14.460 --> 00:56:21.330
To uppercase right and now you've got an Rd where everything has been updated for canonicalization and then you can work on it afterwards. And we didn't see

420
00:56:22.380 --> 00:56:22.770
Life.

421
00:56:24.000 --> 00:56:26.760
Yeah yeah you you could persist at that point. So the first point.

422
00:56:27.060 --> 00:56:34.950
Right. And then that means you're still at that point you know when the thing walks back the chain. It'll never go further back than that because it knows you persisted in the beautiful thing at that point.

423
00:56:35.220 --> 00:56:38.610
So, you know, we would only need to read that text file. The first time to get to that point.

424
00:56:39.450 --> 00:56:44.250
So here, doesn't matter so much because to upper cases, you know, you're already looking at all these lines anyway.

425
00:56:44.910 --> 00:56:58.170
But you know if to uppercase was more expensive. You could imagine. I'm just going to have an uppercase canonical iOS version of this whole book, and I'm going to use that as my starting ground truth and do all these things to afterwards that fit your question. Okay. Yeah, yeah.

426
00:57:03.030 --> 00:57:05.250
Welcome to that next. Okay.

427
00:57:06.990 --> 00:57:17.730
Yeah, oh, this is basically a being really enough memory. Yeah. So there's a better. Well, potentially. You can also use this. Okay.

428
00:57:19.620 --> 00:57:30.690
Which we'll see in a second. So if anything in the memory and a lot of systems are going on different can get pulled away. I'll meet you. Yeah, yeah.

429
00:57:32.130 --> 00:57:34.770
Yeah, but we'll talk more about the details.

430
00:57:36.900 --> 00:57:48.750
So the takeaway on persist, basically, is that it will allow you to take other our DVDs from the one you persisted, but the process of making them starts from where did the persist. Right. You don't have to go back and change earlier.

431
00:57:49.350 --> 00:57:58.080
So if there was something expensive done in there that you expect to be reusing the results of over and over again, you can basically sort of headed off at the pass and keep it keep going back before that.

432
00:58:03.720 --> 00:58:05.730
We didn't get on the

433
00:58:07.140 --> 00:58:19.920
We'll talk about that right now. Okay, yeah, these, these are actually part of questions because like these are they asking is, like, persistent to wear right why, for how long. Um, so when you persisted Priester cash.

434
00:58:20.850 --> 00:58:31.710
There's different storage levels so and locations. So the default is to persist it only to memory, right. So the RD gets stored in memory as D serialized Java objects.

435
00:58:32.490 --> 00:58:41.550
And if the entire already can't fit in memory on that node which is where we're going next. And then some partitions won't be cached and it's just going to throw up its hands up. You know, I

436
00:58:42.090 --> 00:58:48.990
Couldn't do it. I'm just gonna have to repeat it, because you know the cost of computing is probably better than the cost of money going to memory, just the the JVM explorer.

437
00:58:50.040 --> 00:58:57.810
So it'll try but it won't necessarily deliver what you wanted. Right. And that's the thing where you may have to watch the admin tools to look in the admin UI.

438
00:58:58.050 --> 00:59:05.400
And notice that you know that node is running low on memory and it's blowing off your personal requests. Because it just doesn't have the resources to honor so

439
00:59:06.660 --> 00:59:14.820
You know, the default storage level is memory only they're stored as the service objects. The Java objects. People remember what that means is utilized

440
00:59:15.330 --> 00:59:23.700
So a straight out in memory representation of the Java object as it normally exists, and it gets used by the JVM, which is not really expensive, right, because

441
00:59:24.840 --> 00:59:31.170
You know, you think about. So you've got a stream, right, like a string contains an array continue to characters in the stream.

442
00:59:31.350 --> 00:59:41.880
But it also contains some other stuff. Right. Like there's there's an object head around the region Java object that 16 or 20 or 24 bytes or whatever, depending on what type of object is. And there's there's a bunch of junk in there right like

443
00:59:42.660 --> 00:59:49.410
There's fields that the garbage collector uses for keeping track of things. It's done to the object as it's looked at it during garbage collections.

444
00:59:49.980 --> 00:59:55.050
For concurrency, there's like a monitor lock in there for synchronization to work on and stuff like that so

445
00:59:55.320 --> 01:00:02.250
Um, so the D serialized format can be fatter in memory than a good compact serialization format will be because when you

446
01:00:02.610 --> 01:00:11.220
Have a good comeback serialization format, you're obviously never going to call synchronized on some crap on disk or some stuff on the network, right, that you can only do that on a live fully had

447
01:00:13.260 --> 01:00:26.880
Another storage level is memory and disk, where the RD lives as decentralized Java objects in memory. But if the entire it doesn't fit, then it will spill partitions that don't fit onto disk and then return them back as the right so

448
01:00:27.690 --> 01:00:34.860
Is that better or worse than the computing from scratch, right. It sort of depends a little bit on like what the actual work required to recreate the things

449
01:00:35.070 --> 01:00:45.150
Right. So in practice, you might have to if you're coming up against this limit with respect to your cluster resources and what you're doing. You might actually have to look at the lie and say, well, let's spend time on right and if you see that

450
01:00:45.660 --> 01:00:48.660
Spending a lot of time, you know, the serialized or whatever.

451
01:00:48.960 --> 01:01:01.650
And spilling to and from dish, then you might want to think about, you know, mine. She's having a larger cluster, you can spread into more pieces on or, you know, picking your colleagues off of the cluster so that you can monopolize university for yourself or stuff like that.

452
01:01:02.790 --> 01:01:09.990
And then there's a storage level called disco me where, basically, it will sort of put the thing only on disk and then bring it back as needed.

453
01:01:10.830 --> 01:01:25.140
The question came up, but when you call persist met the method persist versus Tash, the short answer on that is cash basically defaults to memory only, but when you use persistence dead, you have the option of saying what which one you really want. So cash default scenario.

454
01:01:26.370 --> 01:01:37.440
And if you really were worried that you were up against the limits of available memory or if you wanted to, you know, potentially take advantage of the ability to store it also to describe spillover access to disk, then you would use persistent and ask for what you wanted.

455
01:01:38.490 --> 01:01:48.120
There's a couple other options memory only serialized in that case, it's been a stored in memory, but the objects will be serialized using three after

456
01:01:48.990 --> 01:01:53.700
Thank you missed by default, but you can also plug it in. So it'll use something like 50 protocol.

457
01:01:54.030 --> 01:02:03.930
Buffers to take that actual Java object representation with its object header and it's GC fields and it's monitor walk and all the other stuff and boil it down into just like a compact pile of bytes.

458
01:02:05.190 --> 01:02:11.490
Again, if it doesn't fit some partitions will be cached not be cached and they'll be repeated on demand.

459
01:02:12.180 --> 01:02:19.260
You can also ask for the memory and disk option with serialization, in which case, it'll store partitions that don't fit and put them on disk as needed.

460
01:02:20.160 --> 01:02:26.670
Obviously there's a bit of extra overhead in here because when you actually do need to operate on one of those objects, you know, say you're getting a string and call length on a

461
01:02:26.850 --> 01:02:35.280
string contains you obviously jogging to call that out right so the serialized form of the string has to be rehydrated and you know resurrected.

462
01:02:36.750 --> 01:02:47.280
The necromancer has to resurrect the serialized form into living stream or sort of living string and then call contains on so there'll be a little, little bit of extra overhead on those. But for simple classes.

463
01:02:48.810 --> 01:02:51.570
They probably serialized the serialized back pretty fast, right.

464
01:02:53.310 --> 01:02:54.630
So does that make sense to everybody.

465
01:02:57.870 --> 01:03:03.360
They can be smaller because they don't have that that Java object overhead. Obviously we

466
01:03:05.250 --> 01:03:06.360
Have a few memory.

467
01:03:08.040 --> 01:03:11.430
Suppose I put something this big and memory versus putting something this big and very

468
01:03:12.720 --> 01:03:13.020
Serious

469
01:03:14.100 --> 01:03:16.590
Yeah. So say, for instance, I had a

470
01:03:17.610 --> 01:03:26.070
Short strings or it's right, you know, an integer really has four bytes of actual information that's the integer. So if I have a four by an integer.

471
01:03:26.340 --> 01:03:35.070
And then I have 16 2024 bytes of ordinary objects header on the top front of it. That means that for every 25 or 30 bites are actually the data I care about.

472
01:03:35.460 --> 01:03:40.470
Right, if I could have saved that ensures four bytes plus another one or two bites for the serialization format.

473
01:03:40.800 --> 01:03:51.330
Or maybe, maybe the serialized are saved. Like did them as a bigger way in which case, there might be a tiny header for the array and then four times. However, many there were verses 20 times however many there were. Right. It can be much more

474
01:03:52.800 --> 01:04:00.270
Yeah, the, the actual in memory size of Java objects is something people frequently don't have any intuition for because Java hides it can be pretty effective. Right.

475
01:04:00.660 --> 01:04:10.020
You know, you can call synchronized on an object and the right thing happens. But like, without further prompting there's no real way you guess how it's doing it right.

476
01:04:10.740 --> 01:04:16.050
And the fact that there there's card marking fields and stuff on the top of the the optic editor that the garbage collector

477
01:04:16.470 --> 01:04:19.500
Will you never see you never see the innards of how that's working right.

478
01:04:19.770 --> 01:04:25.980
You know, the, the only time you ever receive it really is when your machine free stuff because it got into like a bigger major generation collection.

479
01:04:26.250 --> 01:04:32.160
And you're using one of the old collectors. But other than that, you're not you're not aware of it, right, unless it's either really behaving badly.

480
01:04:32.760 --> 01:04:39.540
Or you've had some other reason to be curious about it, like say you're writing a custom garbage collector. So the short answer is the serialized formats can be small.

481
01:04:40.800 --> 01:04:44.490
But there is overhead. When you do need to do something with them. They have to be rehydrated

482
01:04:46.350 --> 01:04:47.340
Let's so

483
01:04:48.420 --> 01:04:55.740
There's also replicated versions. So I can catch a persistent memory with two copies of them for safekeeping. Right.

484
01:04:56.580 --> 01:05:03.930
And those are the same as the ones on the previous slide, but everything is just replicated twice, and now we're at 707 so it's probably a good time to

485
01:05:04.530 --> 01:05:16.500
You know, on, on All Hallows Eve honor the ghost of take over items probably haunting the streets of Prague looking for a bathroom, even as we speak, hopping on one foot is gold nose being shaken loose.

486
01:05:21.870 --> 01:05:23.250
Yeah, so

487
01:05:29.850 --> 01:05:31.410
Happened to happen. So each of the partitions. Right.

488
01:05:32.670 --> 01:05:33.090
Yeah.

489
01:05:34.590 --> 01:05:43.170
Yeah, because you think about you know what this clusters trying to do is provide a view of this thing to you as a single unified logical object. Right, right. So when you ask for this to happen.

490
01:05:43.410 --> 01:05:48.150
The logical thing to do is to do it on all the pieces in all the places where they happen to live.

491
01:05:48.870 --> 01:05:52.620
And of course, you might have nodes Lino say for instance you had some things that shuffled

492
01:05:53.100 --> 01:05:59.460
To me before. Not all nodes might necessarily the end of the same memory pressure, like, you know, I think it may have been trouble different ways they need

493
01:05:59.700 --> 01:06:06.630
A partition or coalesced in different ways. So you may end up with a case where like one node, like everybody else has breathing room to do this, but I felt

494
01:06:06.870 --> 01:06:12.510
Right, and that's the case where there's other options for spilling to do score. You know, do the best you can. And if you can't just don't

495
01:06:13.530 --> 01:06:22.800
Might come into play and they might not come into play. Exactly the same way in all the nodes, because not all the nodes are necessary and facing the same environmental resources and constraints that make sense.

496
01:06:26.910 --> 01:06:27.870
How long is the big

497
01:06:30.270 --> 01:06:34.050
40 seconds I expect people are getting pretty fast at this by now. No, no.

498
01:06:40.350 --> 01:06:42.750
Why did it come back.

499
01:06:49.470 --> 01:06:50.040
Hopefully,

500
01:06:53.130 --> 01:06:53.460
And

501
01:06:55.020 --> 01:07:06.060
I'm very excited. Exactly right. Right. And and the, the actual, you know, driver know that's coordinating through the things it's really just, you know, doing. It's kind of the ringmaster it's it's not really

502
01:07:07.620 --> 01:07:14.520
Yeah, yeah. It's job is basically to figure out what work needs to be done, and where it needs to be done and just kind of throw it over the wall and, you know,

503
01:07:15.540 --> 01:07:16.740
Hardest results to come back.

504
01:07:21.630 --> 01:07:29.070
With implementation only as good, too, because it means that all the complex smart stuff is kind of close to an edge of the system. Read the big loss deeply in some

505
01:07:30.240 --> 01:07:30.900
Does the

506
01:07:33.630 --> 01:07:34.710
Yields already

507
01:07:35.760 --> 01:07:44.100
Um, yeah, I mean, all these things have to understand these things well enough to do their part in either maintaining or creating pieces of them or vindicating or access

508
01:07:45.180 --> 01:07:48.330
Right so yeah and and

509
01:07:50.640 --> 01:07:58.260
Because you made those diagrams from the first slide, we had on spark where we had like a couple ways of deploying like there was the standalone single node version. And then there was the node where you have the driver.

510
01:07:58.530 --> 01:08:05.460
litmus basically in the replicable spark shell and then you had the driver that space static we created inside a scholar Java program to talk to the same

511
01:08:06.510 --> 01:08:20.160
And so you have this piece that lives near you, which is smart enough to talk to the the stuff on the cluster server end right and then from that, you know, get the data moved up partition your get jobs scheduled and running right places.

512
01:08:29.400 --> 01:08:29.820
Dax.

513
01:08:32.130 --> 01:08:33.240
Um, yeah, we're gonna

514
01:08:35.040 --> 01:08:41.970
We're gonna look at the while the web UI. A little bit. A couple corners of it later tonight and you'll see what the daggers end up looking for both

515
01:08:43.500 --> 01:08:57.150
Natural spark jobs just done the regular way. And you'll also see what the day eggs that are produced at the back end of spark SQL look like and the spark SQL SQL ones, it's relatively easy to see it producing more complicated ones. I can see if I can use good example of what

516
01:08:58.590 --> 01:09:01.440
I was doing something with one earlier today that might still be around.

517
01:09:11.220 --> 01:09:14.430
About right between 10 and 20 minutes 15 is probably about a typical

518
01:09:15.390 --> 01:09:27.300
So usually, before I start using it. I look at that progress bar. Yeah, because you know when you jump in and start using it before. It's all there. It's not very pleasant right day you know you you get errors and they're not

519
01:09:28.350 --> 01:09:31.170
They're not very attractive. Um, let's see. So

520
01:09:32.340 --> 01:09:34.110
Did I close it. I might have

521
01:09:36.060 --> 01:09:36.840
Already

522
01:09:42.720 --> 01:09:46.680
started kicking around but maybe it may have gone away, of all things.

523
01:09:48.180 --> 01:09:48.960
Will make a new one.

524
01:09:53.700 --> 01:09:54.600
He

525
01:09:58.740 --> 01:10:02.160
Yeah, apparently disposed to the carelessly, but I will will make you

526
01:10:43.890 --> 01:10:45.000
Live from the

527
01:10:48.600 --> 01:10:49.140
Bob

528
01:10:51.090 --> 01:10:52.020
Playing my stuff.

529
01:10:53.700 --> 01:10:57.270
On the house is worth. So that is the life science.

530
01:10:59.130 --> 01:10:59.700
Yeah.

531
01:11:01.320 --> 01:11:03.270
So how do we do fighting

532
01:11:05.400 --> 01:11:06.090
HD

533
01:11:09.630 --> 01:11:19.830
Yes. Yeah, just use the one of the simplest thing I was saying it's sq ft underscore living and that's the one you want, because there's also underscore living 15 and La

534
01:11:20.910 --> 01:11:23.190
And and I actually looked at the file like

535
01:11:23.910 --> 01:11:38.550
A either way down there like columns 29 and 30 or something and be the numbers are there are often similar to the other two but a little bit bigger, but not in a way that obviously suggests anything. Just, just use square foot underscore living and and the first ones you see

536
01:11:40.020 --> 01:11:48.870
Because there's other ones. It's maybe there's some weird realtor term that square foot 15 or his foot 15 inches. But even that doesn't numerically protocol. Right.

537
01:11:50.580 --> 01:11:54.090
Because the difference is not big enough to to possibly for that so

538
01:11:59.850 --> 01:12:05.580
Oh yeah, go ahead and go ahead and put in the back end and you know if you misinterpreted and took one of these random columns.

539
01:12:05.820 --> 01:12:16.080
Like you won't be punished for it because i mean you know it's not clear what they are and be like the results. Most of them aren't that different right there will be like 30,008 versus, you know, 3100

540
01:12:16.680 --> 01:12:23.640
Right, like, you know, basically, it's an extra bathroom or like an extra creepy crawl space somewhere.

541
01:12:26.130 --> 01:12:31.800
Yeah, yeah, you've got a, you know, some kind of Silence of the Lambs things going on with the basement. Right.

542
01:12:33.780 --> 01:12:36.600
It puts the square feet on its skin or else it gets the hose again.

543
01:12:38.160 --> 01:12:39.150
Anyone seen that movie.

544
01:12:40.710 --> 01:12:46.350
Silence of the Lambs and won Oscars. Yeah. OK. So the joke just wasn't fun

545
01:12:52.320 --> 01:12:53.310
Good cover good cover.

546
01:13:06.810 --> 01:13:07.110
Oh,

547
01:13:08.550 --> 01:13:19.230
Yeah, so be careful of the ones I uploaded the canvas are the ones to use because there's another set of airport data floating around that looks superficially similar. But isn't the same one.

548
01:13:22.050 --> 01:13:26.490
Are they actually different. I think there might be two copies of the same one I made it repeated in probably

549
01:13:31.140 --> 01:13:39.180
Oh yeah i think i think i might actually say to twice a day when I went back to find the files. I went back to the place I thought I we found the files originally

550
01:13:39.510 --> 01:13:44.640
And I got them and nothing worked. And like, there aren't the same number of columns and some of the columns have different names and

551
01:13:44.910 --> 01:13:52.620
And but a number of the gutter comes calls are similar like at first glance. They almost look like they work, but they don't work with our slides. So I went back and found the ones from last quarter.

552
01:13:54.120 --> 01:13:57.780
And now the original source of the ones from last quarter. That's just lost the ages at this point.

553
01:13:59.130 --> 01:14:02.370
So if they ever get lost all all the slides have to change.

554
01:14:13.080 --> 01:14:19.590
Oh, that's not exactly sure there's a Jupiter in the head in the chat box.

555
01:14:20.640 --> 01:14:23.730
There is, oh, that's how we can be angels

556
01:14:25.020 --> 01:14:31.860
That that is a separate docker container independence or sandbox. Yeah. Okay. Yeah, yeah.

557
01:14:37.590 --> 01:14:38.490
Yeah, the next thing.

558
01:14:40.110 --> 01:14:40.440
Oh,

559
01:14:43.680 --> 01:14:55.350
Yeah, I didn't know. I didn't know I DID NOT KNOW WHAT DID THAT THAT IS A GOOD. THAT IS A GOOD MORNING for people to have, I mean nothing in that notebook is really for the ages. Anyway, so it's all reproducible from from what's there, but knowing that knowing that it

560
01:14:55.830 --> 01:14:57.300
You don't want to do new work in it.

561
01:14:58.350 --> 01:15:05.970
For doing new spark work, you probably want to use the Zeppelin notebooks in the sandbox will will will visit those later today, or if not next week.

562
01:15:10.290 --> 01:15:10.620
Yeah.

563
01:15:11.700 --> 01:15:21.120
Yeah, no, it's great. This world we live in, like, I remember years ago, like there was like oh reuse is important. We must have one copy of everything and strictly version. So everything could use it and share it.

564
01:15:21.450 --> 01:15:25.470
And then it was like, oh my, and then the problem with DLL hell. Right, it's like, Oh, you've got

565
01:15:27.570 --> 01:15:41.670
Wah wah Buddha Buddha dot DLL some version and some other different version two things that machine now are dependent on bugs in the different versions and now everything we use you know Maven IV Anaconda

566
01:15:43.470 --> 01:15:43.980
Pip.

567
01:15:45.150 --> 01:15:59.250
Python snake eggs to whatever they're called Ruby Gems, we now own these environment magic managers like Anaconda and our VM and Maven and all these other things that basically start every day with every project by downloading the whole goddamn thing.

568
01:15:59.940 --> 01:16:10.260
And always keeping one copy of everything you've ever used so that whenever you use it will always have it forever because we've decided we could never actually make any two things work together twice ever again.

569
01:16:11.640 --> 01:16:20.430
And it's more, it's much cheaper to just have a giant hard disk full of 47 billion versions of the same thing all slightly different that you can't even tell apart.

570
01:16:20.640 --> 01:16:32.970
That you can kind of keep together with like, you know, duct tape and string in a version combination you saw work one time, hopefully for and even that isn't enough, they still basically break just

571
01:16:34.170 --> 01:16:36.060
You know, well, in order to not even running

572
01:16:39.210 --> 01:16:41.430
too vast and trunk legs of stone.

573
01:16:42.630 --> 01:16:44.910
Everybody is just a big Ozymandias crap.

574
01:16:48.120 --> 01:16:54.840
I mean like the world is full of abstracted carefully maintain software that you should trust and rely on all the time.

575
01:16:56.040 --> 01:16:57.690
Tonight didn't recover that quickly enough.

576
01:17:01.860 --> 01:17:02.370
Said,

577
01:17:06.300 --> 01:17:07.770
I know they give me

578
01:17:09.330 --> 01:17:21.480
What's biting you is not the spark show. I think it's actually route login session on the sandbox. Yeah. And the answer is yes, you should be able to. But, uh, but I failed.

579
01:17:22.290 --> 01:17:33.150
And so the first thing I thought is, okay, well, it's just you know they're there is an environment variable that tells batch. How long could let the session, be it will be for timing you out. So I said that and it didn't do anything. I still got time.

580
01:17:33.660 --> 01:17:39.330
And then, you know, SSH the SSH server, it has a very well as well, which you can figure that will decide how long

581
01:17:40.050 --> 01:17:58.410
Before it locks you out and I changed that and I still not long ago, so I I made one or both of those changes incorrectly by, like, you know, not realizing the timeout is even milliseconds to seconds or something. Right. But I said it like 2700 just like 45 minutes right like that, you know,

582
01:17:59.430 --> 01:18:05.580
Like a would never let you never take milliseconds, because who wants it to timeout after 2.7 seconds. Let's just say that would never be

583
01:18:06.150 --> 01:18:14.760
But when I said it's like 45 minutes. Still got kicked out after like two minutes or whatever. So the two things that I thought it could be it either botched times changing them.

584
01:18:15.600 --> 01:18:21.600
Or there's something else that that's timing out and then I got distracted, get back to it so

585
01:18:22.230 --> 01:18:27.240
I'll try to make a note to look at it again because I know it is annoying when you stop to meet documentation and come back.

586
01:18:28.170 --> 01:18:36.510
You've been kicked out which, you know, those, those things are a great investment get one of those and put it like next to the Shift key or something where the spacebar.

587
01:18:38.100 --> 01:18:38.430
And

588
01:18:40.020 --> 01:18:43.920
I'm not suggesting you buy those but you know for this purpose to possibly

589
01:18:46.140 --> 01:18:47.760
One other thing that I also

590
01:18:50.100 --> 01:19:04.560
Like maybe since last week, the year, the year of time. Yeah. Like, as you're starting slower than it used to. Oh, hey, Christy, are we thinking about English

591
01:19:05.310 --> 01:19:19.170
And that's for the VM, not for the sandbox. Yeah, definitely. Okay. Yeah. It takes time. Because the Lord all that. Yeah, and it's like a 20 minute job yeah but the but the VM isn't ready yet. Like, you know, what does it only makes it should be a two or three minutes.

592
01:19:21.990 --> 01:19:31.260
And it's consistently, you know, not interesting. Yeah. I wonder if I wonder if you're just running on a host that happens to be really busy.

593
01:19:32.310 --> 01:19:32.850
Which

594
01:19:33.960 --> 01:19:36.780
If it keeps doing it gets worse, send me email, it'll

595
01:19:37.920 --> 01:19:47.190
Ask about it. It's possible that you know our resource group is kind of bulging at the gills if if a bunch of us are busy at the same time. Who knows whether we're all being scheduled at the same piece of physical hardware.

596
01:19:47.580 --> 01:19:51.360
Right, that's one problem with with the virtualization cloud stuff, right, is that

597
01:19:52.890 --> 01:19:53.400
The

598
01:19:54.960 --> 01:20:06.390
You know, it's like living in an apartment with noisy neighbors, right, except you. You don't really know their whole directly just realized that, you know, there's no water in your shower and you haven't slept in three months.

599
01:20:07.980 --> 01:20:08.460
Sure why

600
01:20:10.260 --> 01:20:20.970
But with the neighbor case, you actually have a better idea. Okay, so we have everybody back in the room and online people like presumed had ample time to commune with the spirit of to Brian

601
01:20:22.050 --> 01:20:25.890
Perform whatever helming rituals are needed. Okay, so let's let's keep going.

602
01:20:26.910 --> 01:20:33.600
So the last bit of wrap up before we move on from a persistence and caching is that at some point maybe you want to let go of this

603
01:20:33.990 --> 01:20:45.840
And the answer there is an persist. So if you called cash or you called persist and persist specifying some stories level are called cash to default to that most basin and primitive stores levels.

604
01:20:46.380 --> 01:21:02.400
You you let go of an Rd that's been you let go of a cash versus the RGB by calling on persistent and then it goes back to the same state. The already already have, which is if it's needed again regenerated again when you pull up action. Okay, so

605
01:21:05.490 --> 01:21:21.930
Yeah. OK, so a spark SQL. Um, so in the questions. People are asking in class and the word question people have been asking email and the comments, people have been making on like some of the slightly more challenging parts about using the raw Rd API, which you're you're

606
01:21:23.550 --> 01:21:34.200
Asked to do for the week three assignment is to, you know, get a feel for our DVDs, then we'll move on to some of the more high level friendly tools and the sort of center that university so called spark SQL.

607
01:21:35.730 --> 01:21:36.150
The

608
01:21:38.370 --> 01:21:43.620
In Week four last year, I think we stopped here. So we're actually a little farm further behind where we were.

609
01:21:45.090 --> 01:21:52.080
Last year, but that's okay because you know we've covered some other stuff. And we probably have less backtracking to do later. Um, so where are we going so far.

610
01:21:53.010 --> 01:22:01.980
We've covered already these it's something we talked about what they are, how they exist physically in the cluster how compute work happens on them, how the storage is broken up and partition can spread across nodes.

611
01:22:02.970 --> 01:22:11.400
You know transformations that build our movies from previous armies and actions that pull on a chain of our DVDs to make some kind of computation actually happened and generate some kind of results.

612
01:22:12.300 --> 01:22:20.940
So now we're going to move on to talk about spark SQL this diagram I don't quite know why this year like these slides like Jason had like five versions of this slide with like

613
01:22:21.450 --> 01:22:31.230
Disappearing and going away. But I think regardless of narrative original version of the slides and I never really understood the narrator was so I just left this cryptic placeholder here.

614
01:22:31.530 --> 01:22:39.150
And the remaining five slides of like lines edges disappeared and moving around. They're not there. So if you've been looking at stripe trying to understand it.

615
01:22:40.350 --> 01:22:40.710
You know,

616
01:22:42.420 --> 01:22:43.080
Yeah, don't

617
01:22:44.970 --> 01:22:51.240
Don't even try to understand anything until I've said it's worth understand. No, that's not a good policy. Um, so, anyway.

618
01:22:52.170 --> 01:22:59.460
So some of these limitations already do right, is that, you know, there's no other type of information right if you pull an odd and TXT file and you basically get

619
01:22:59.850 --> 01:23:07.410
an RPG, which is a sequence of streams each stream corresponds to a wine. What's in that line is God only knows and you know maybe you break it.

620
01:23:07.710 --> 01:23:15.300
Comma Separated you chop it into PCs and you index in the columns and do things with them. But you know, that's kind of crude and primitive and clumsy and stuff so

621
01:23:16.320 --> 01:23:24.900
making things better than that lead to things like spark SQL, which was a similar question or and I just listening. I thought I heard his voice.

622
01:23:27.210 --> 01:23:27.570
So,

623
01:23:28.800 --> 01:23:38.580
Initially people tried to get high for spark and people didn't get hype working on spark and then that led to a thing called Shark, which was high on spark and awkwardly named

624
01:23:39.270 --> 01:23:47.160
And then what was originally shark, which was hive on spark moved sort of more towards the core of the spark system and it sort of got rebooted

625
01:23:47.640 --> 01:23:49.710
And so that history is basically an interesting

626
01:23:49.890 --> 01:23:58.260
Right. The thing you want to learn anything company called spark SQL and I only mentioned that history is because as you go around if you stumble across all versions, the documentation will see references to shark.

627
01:23:58.350 --> 01:24:05.790
And Chuck Q Well, and these other things. And these are basically like gone now post version 1.4 I think spark.

628
01:24:06.300 --> 01:24:14.190
That stuff that's what all the major changes happened and everything we're doing in this class. And that you'll probably do in your life going forward will be spark to or something later this part two.

629
01:24:14.550 --> 01:24:24.120
And all that will sharky stuff is now st beneath the waves and so spark SQL is going to be looking at. Just be careful when you're looking at older documentation. If you do stumble across stuff. It's pretty one point for

630
01:24:24.360 --> 01:24:31.890
You may see stuff that's just similar enough to what you're wanting to look tempting, but just different enough it will be useful.

631
01:24:32.610 --> 01:24:44.460
So avoid those. So what is marked as you're getting right so it gives you a way of dealing with structured data, or at least data that's more structured than what we've seen so far that lends itself to sort of row column format.

632
01:24:45.330 --> 01:24:49.800
It's not necessarily as relational as a relational database. In fact, that it really isn't.

633
01:24:50.130 --> 01:25:00.450
But it has an a structure and enough infrastructure built around it that some of the optimizations that relational databases have historically been able to do it can do some of them and it's growing more than over time.

634
01:25:01.170 --> 01:25:08.100
And it also stores its data off of the JVM heap. So when it does things like joins, you can actually do the more efficiently, because it's working on a more

635
01:25:08.460 --> 01:25:12.030
Efficient in memory representation that it's managing through Jay and I or whatever.

636
01:25:12.360 --> 01:25:20.610
Then you know arrays of Java objects with all the headers and stuff. And I never used to because a lot of those fields headers if if your data really is the value of class.

637
01:25:20.970 --> 01:25:26.280
Right, and it's immutable, why even have an object monitor synchronization lock on it, right, like

638
01:25:26.670 --> 01:25:36.840
Data never changes is something you really don't have to worry about like right conference on. Right. So some of the overhead that comes with D serialized Java objects as the representation in memory or somebody things

639
01:25:37.350 --> 01:25:51.360
Really doesn't actually serve any useful function when you're emphasizing the ability and eventually project Valhalla, or whenever will probably add the classes or something like that to the JVM and the JDK so you'll have objects like that in Java itself.

640
01:25:52.800 --> 01:26:00.210
Which would be great for certain applications. So anyway, here's the architecture of the sparks equal system. So you've got

641
01:26:00.720 --> 01:26:13.320
Spark SQL data. And so from spark SQL Data you'll perform some kind of query and so spark SQL will have to produce some kind of logical plan for how this how this query workout right scan this table. Look at this index, you know,

642
01:26:13.770 --> 01:26:20.160
join these two results by, you know, setting up a hash to figure out which pieces go together with one another. And then you have that logical plan.

643
01:26:20.670 --> 01:26:22.560
And then there's this optimizer thing.

644
01:26:22.950 --> 01:26:31.950
That basically works on the logical plan to try to figure out if there are options that could be strength for you still may be rewarded for efficiency, based on the size of things or whatever, you know, the sort of standard

645
01:26:32.520 --> 01:26:42.360
Sequel compiler type optimizations and you know data access like lobster optimizations that aren't relational databases have done forever. And then there's this this execution engine that takes

646
01:26:43.800 --> 01:26:46.530
The original version of the spark SQL used lots of chunks of hive.

647
01:26:47.370 --> 01:26:51.810
In kind of a clumsy and cannibalistic way but now they mostly rewritten, all that stuff for themselves.

648
01:26:52.080 --> 01:26:59.520
And so when you are using hive in the context of spark today, as we'll see later. Typically you'll use what looks like the high front end with

649
01:26:59.730 --> 01:27:09.570
Hive Q AMP L or higher version of sequel, but it's actually going to run the query top spark using this mechanism here rather than the, the original certain of hive meta store stuff that you saw in

650
01:27:10.920 --> 01:27:17.700
Week one, I guess it was okay. So what does Spark SQL give you as the user right so

651
01:27:18.180 --> 01:27:19.950
We mentioned earlier that you know

652
01:27:20.310 --> 01:27:30.210
And and some of your complaints were that, you know, we have this data and you know the columns, there are types, but we're still accessing them by position and they're always strings until we turn it into something else and

653
01:27:30.390 --> 01:27:39.000
We hopefully grabbed the right thing and turned it into something that can be turned into where we get cast class exceptions or other problems like that are you know integer number format exceptions or whatever.

654
01:27:39.330 --> 01:27:43.980
And you know that that sucks, right. So you'd like to have that that typing that.

655
01:27:44.370 --> 01:27:51.600
That type system information actually do something for you. It's more pleasant than raw RDS which may be just the most basic primitive crude type

656
01:27:51.900 --> 01:28:07.860
That you can have to sort of massage records and you can actually work with so spark SQL introduces to new creatures that the data set, which is sort of more closely tied to Scala objects and that will get to that later. And it should work through the scala

657
01:28:09.000 --> 01:28:16.500
Training notebook. You'll, you'll, you'll be well prepared to think about that as well as data frames and so has anyone in the class use our

658
01:28:18.360 --> 01:28:24.930
Okay, so we've had a couple our users. So, you know, so the term data frame is familiar from our when we talk about data frames and spark SQL.

659
01:28:25.590 --> 01:28:30.900
The mental picture you form of our data frames. That's the thing that spark SQL is imitating or eating.

660
01:28:31.410 --> 01:28:38.130
Or parody. Parody or or whatever, that's kind of the conceptual inspiration for sparks different

661
01:28:38.880 --> 01:28:48.330
So the thing on the left is example was something kind of different, right, you've got something that's basically it looks like a table made on the bros, the columns have names and types associated with them, just like in our

662
01:28:49.020 --> 01:28:56.370
Data Frame associated points you're using for linear regression, whatever. And the thing on the right is multi data center where you've got basically Scala objects.

663
01:28:56.940 --> 01:29:07.710
In this case of type of car right and then you got a collection of those things and you can do like Scala code type things with them. It turns out that the performance between data frames and data set is actually surprising because

664
01:29:08.970 --> 01:29:17.610
But the the sort of short mental cartoon picture to keep in mind is basically the data frame is the thing where you have columns, you've got a cable native rose the columns have names and types.

665
01:29:18.270 --> 01:29:28.530
And then the the spark data set is basically a array of scholar objects to have a certain type that we'll talk about later and and types that you can actually define

666
01:29:29.580 --> 01:29:40.500
So what we'll do data frames first because there's less scholar type stuff to think about in those. So let's start up a spark thingy and do some stuff with the data.

667
01:29:42.330 --> 01:29:49.380
Okay. Oh, and my thing will be dead. So I'll probably have to, oh no it's not dead. It's fine. Alright, so

668
01:29:58.170 --> 01:30:02.370
Dear frames and data sets are both implemented on top of our DVDs, so

669
01:30:04.080 --> 01:30:12.600
Yeah, underneath them underneath them are DVDs or the machinery that make them work. And then there's some extra metadata machinery associated with them to give them some of these extra nice properties.

670
01:30:13.710 --> 01:30:15.630
And so let's get our show window opening here.

671
01:30:16.800 --> 01:30:18.480
What was my idea was that

672
01:30:19.740 --> 01:30:22.800
It was definitely that. All right.

673
01:30:25.890 --> 01:30:26.160
OK.

674
01:30:29.130 --> 01:30:32.730
OK. And then let's go

675
01:30:34.500 --> 01:30:37.140
Get a session start. So let's start up the spark shell.

676
01:30:39.420 --> 01:30:40.440
that's big enough for everyone to see.

677
01:30:46.800 --> 01:30:51.360
And lose the spark session, which I think will get by default.

678
01:30:54.780 --> 01:30:55.890
Takes a while to start

679
01:30:57.630 --> 01:30:58.650
Way to start

680
01:30:59.700 --> 01:31:01.350
What do you say about your machine being snow.

681
01:31:03.480 --> 01:31:13.980
So we do have a spark session here. That's good. So most of what we've done so far we've used the spark context, which was the old way a driver program or the rebel or the command line talked to spark.

682
01:31:14.490 --> 01:31:22.740
So, this thing's a sea of tiny spark context. There's also a thing called the spark session, which is sort of the newer notion that does more or less the same types of stuff.

683
01:31:23.940 --> 01:31:26.670
And if you look at the properties, it has on it.

684
01:31:27.870 --> 01:31:35.940
There's a bunch of stuff there. So, well, this is just stuff and inherited from Java object. Remember I was saying about optic headers. Although, in this case, maybe you actually care about some of them.

685
01:31:37.830 --> 01:31:46.080
Yeah, those things and and se was the thing I don't want to have that. And that's the thing we've been calling so far for things like

686
01:31:47.220 --> 01:32:05.790
St dot txt file and so on. So we'll get the you will use the spark session. So let's create a data set. So let's call it DS and we use the spark session. And let's say create data set. And let's just create it for Scala collection. In this case, arrange

687
01:32:07.590 --> 01:32:09.540
And it'll do it.

688
01:32:10.740 --> 01:32:11.940
Let's see what it comes back with

689
01:32:12.960 --> 01:32:25.350
Let's hope it comes back with something first time it's usually a slow. Okay, so we got back and object to type org Apache Spark SQL data set of integer. That's not terribly surprising. And if we do do this show, say the first 10

690
01:32:26.580 --> 01:32:32.070
We get one that's, you know, okay, the pretty printing makes it look a little bit better than it deserves to

691
01:32:32.700 --> 01:32:40.110
But you'll see we got a column. And it's obviously a column, because it's got like a nice box around like that's that's totally different than what we saw before. No, actually it's not

692
01:32:40.560 --> 01:32:49.380
I mean, there's no reason to believe that it's but it is. So, the point was when we asked the data set to show us show us what you got that episode.

693
01:32:51.090 --> 01:32:59.820
He showed us this thing and it said, well, okay, this column has the name right and it has these values and we know how many rows. The thing has and that sort of makes right

694
01:33:01.500 --> 01:33:04.560
So that's fine. Let's move on.

695
01:33:08.610 --> 01:33:15.270
Okay, so how do you construct data set. So remember their similarities

696
01:33:15.780 --> 01:33:27.240
You can construct them from RDS you can construct them from straight out scholar collections like that thing. We just did review the data set from a collection, which was the same as doing spark context paralyze some spark collection.

697
01:33:28.200 --> 01:33:37.260
And then how did it know the stuff about types and things like that. Right, so it can either use Java reflection, whose is everybody familiar with the Java reflections.

698
01:33:38.010 --> 01:33:49.080
I see. Generally not. Is anybody not familiar with a generic section is up to a couple people are not familiar, okay. So when you hear reflection in the context of Java or in the context of other programming languages that use the same thing.

699
01:33:49.590 --> 01:33:58.470
Reflection is basically a set of API's or tools that allow you to ask questions about the architecture program. Right. You could go to an object and say, you know what classroom or you could

700
01:33:59.100 --> 01:34:03.120
Say What methods do support right or you could go to Object can say what are your fields.

701
01:34:03.720 --> 01:34:12.180
And you know in Java modulo potentially security constraints, where there are certain things you shouldn't be able to know about Apollo reflection on you basically drive a truck through all of them. If you, if you, if you do it right.

702
01:34:12.990 --> 01:34:19.800
It allows the basically programs circuit examine their own data and make inferences about it. And so you can see how, in a lot of cases where

703
01:34:21.570 --> 01:34:28.110
You actually new data about these types that was actually stamped into the types themselves in the program, you know, something like spark could look at it and say,

704
01:34:28.470 --> 01:34:32.910
These things class. It's got a couple data fields on it. And there are these types. And so that's what we mean by reflection.

705
01:34:33.510 --> 01:34:42.450
That sense of it, much like you might reflect on your, your, your inner thoughts or your past mistakes or whatever. That's what reflecting reflecting on the nature of the objects.

706
01:34:43.860 --> 01:34:48.300
So you can create groups that I lose this thing I did.

707
01:34:50.910 --> 01:34:51.600
Yeah, so

708
01:34:53.280 --> 01:34:55.800
So you can construct a sentence, a couple ways. We just saw one

709
01:34:56.880 --> 01:35:02.490
And, you know, there's two ways to serialized these things by default spark use Java serialization

710
01:35:03.600 --> 01:35:15.480
And remember serialization is the process of taking an in memory live Java object and converting it into a nurturing of bites you could write on paper on disk or send over the network or whatever and D serialization is taking that serialized form and pulling it back.

711
01:35:15.780 --> 01:35:23.280
And serialization is awful name because it it's a nonstick it was introduced in Java. I think it was Jim all those fault.

712
01:35:24.420 --> 01:35:33.720
And the people who use serialization and concurrency for years were annoyed from the workshop is appropriate because it got used to him adjacent introducing context and people complaining about it but

713
01:35:33.960 --> 01:35:41.190
You'll just have to remember that when you talk here about serialization in the context of serialized ability or serialization the context. Currently, it doesn't miss it means a different thing.

714
01:35:41.970 --> 01:35:45.150
Which you can learn a button different course. But anyway, by default.

715
01:35:45.810 --> 01:35:56.070
Spark datasets use Java serialization, you can also use a framework called cryo which is much faster and it has some limitations Java serialization has turned out not to the beach. Very well.

716
01:35:56.940 --> 01:36:02.430
And we'll take a brief aside because you'll see that the use of serialization versus these other things.

717
01:36:02.640 --> 01:36:14.250
We won't really go into stuff in this class that requires you to grapple with them in detail, but knowing that they're there, and that it works this way and it will probably encounter people using other serialization format strict protocol buffers cryo etc.

718
01:36:15.120 --> 01:36:23.790
Avro because of some of Java serialization discontent some shortcomings, that's the sort of take a lesson one, one of the promises Java serialization is a price to be to be gentle.

719
01:36:24.210 --> 01:36:33.030
Right, is that you can have a Java class, and it's taken have a bunch of fields that are actually pointers to instances other classes. Right. And then those component to other instances of other class.

720
01:36:33.390 --> 01:36:43.020
And then somehow you get a point you're going back into the one of the one of his predecessors. So now you've got this graph of Java objects that are all pointing to one another. And it's got the cycles and it looks back on itself.

721
01:36:43.440 --> 01:36:50.130
And so that very first object we started that I now go to the serialization for you to take okay serialized that I want to write that on as

722
01:36:50.910 --> 01:36:57.390
Well, okay. It knows where to start and then that first top of the things that that first objects referring to. It knows where to go to next.

723
01:36:57.750 --> 01:37:08.460
It knows where to go to next. But if ever wants to finish. It's going to have to make sure that deal with the possibility that it might cycle back right and you know that's that's extra complexity complexity goes to go and serialization framework.

724
01:37:09.840 --> 01:37:20.280
Then there's also certain fields, you might never once you realize, right. Maybe they're only associated with like an in memory temporary ephemeral representation of the object that they're not really for the ages. So you have to mark them special

725
01:37:21.330 --> 01:37:30.690
And, you know, then when when an object is the serialized and created there are cases where basically the object will be brought back into existence. Well, calling any of its constructors.

726
01:37:31.170 --> 01:37:37.350
So if someone had a bunch of bikes that they serialized to disk and they knew what process, you were going to use the D serialized them.

727
01:37:37.920 --> 01:37:49.230
Well enough, they could write a malignant sort of pathological pile of bytes to disk. They're going to disappear is something that you're not expecting and maybe take over your crash or take over your Java Virtual Machine right so serialization

728
01:37:50.490 --> 01:38:01.170
It's not very JavaScript annotation. The default when it's not very fast because it has to support all this generality, most of which you don't care about because most of the serialization use cases and the thing like spark are I've got a giant

729
01:38:01.590 --> 01:38:13.110
Giant sequence of objects that are all the same type. They are immutable, all they are static data just write them down somehow right. Don't worry about them piping pointers that refer to each other because they don't think they can. They can just write that down.

730
01:38:13.380 --> 01:38:15.930
But no data serious he still has to be correct.

731
01:38:16.770 --> 01:38:27.780
And then this sort of extra linguistic object creation, where you have to create an object for it serialized form the object might have capabilities in generality that are exploitable by, you know, a pathological example of that form.

732
01:38:28.380 --> 01:38:35.760
version is kind of an awkward because you have to sort of do it manually right where you write the classes and stamp a version on them every time you change them. You have to put a new version on them.

733
01:38:36.000 --> 01:38:39.750
And the thing that's serious decentralizing has to check and it's easy to forget.

734
01:38:40.620 --> 01:38:49.050
And have the wrong version on it. And then, in which case the thing will be serialized just in the garbage, right, because you know it's going to hold the shape and you know that whole list real version.

735
01:38:49.290 --> 01:38:54.240
And so the new thing is either too big or too small. And so well. Something's got to give so it's

736
01:38:55.170 --> 01:39:08.040
So cryo can be much faster, but it's much simpler. On the other hand, it won't actually deal with every possible Java type that is quote unquote Park, but the Java interface serialized novel and you have to register the classes you will use in advance.

737
01:39:09.120 --> 01:39:10.980
And some of these things are flexible and configurable.

738
01:39:12.090 --> 01:39:17.130
You got a civilization was heavily used at one point and things like Java or am I, and some of the Corbett implementations

739
01:39:17.370 --> 01:39:24.870
But those are dying off and if Java serialisations just outright removed from the JDK at some point, it wouldn't be surprised if, in fact, people have been talking about it for years.

740
01:39:25.050 --> 01:39:31.260
And the first moment they think that the use of them has dwindled enough. I think they will get cold because they're there more problems than usual.

741
01:39:33.210 --> 01:39:37.890
cryo isn't the default in Spark. Because of this, this custom registration requirement.

742
01:39:38.520 --> 01:39:45.750
But if you are writing applications that do a lot of stuff that all moving stuff over the network, either to in from other systems or or or shuffling or whatever.

743
01:39:46.440 --> 01:39:51.480
Then basically, you probably want to consider, you know, reading the documentation and using instead

744
01:39:52.320 --> 01:39:59.430
Spark internally since two point O uses cryo for shuffling our DVDs with simple types arrays, a simple types and strings.

745
01:39:59.640 --> 01:40:06.360
Because those types of curse so often. And there's a small finite number of them, so it can deal with the registration problem on its own. And it does that by default.

746
01:40:07.080 --> 01:40:10.650
And over time, it might actually deal with other ones as well. But the takeaway from this is

747
01:40:11.130 --> 01:40:21.180
All of this stuff has to be serialized in DC realized at some point. And the question is, what mechanism is used to do it right and there's at least two commonly used mechanisms here, one of which will someday probably go

748
01:40:22.380 --> 01:40:25.170
Okay, so back to creating a data sets.

749
01:40:26.490 --> 01:40:30.570
Let's just mess around and do a couple. So that was data frames, we

750
01:40:31.920 --> 01:40:36.540
Were on data feeds the data sets. What did he say, Well, that's interesting. I haven't

751
01:40:37.590 --> 01:40:51.570
The opening for a separate site for the sections has data frames and then we're talking about data sets. Okay, fine, whatever everything I said remains true and applicable. But, you know, if I said data frames four times that I should have. That's, that's just pretend I didn't. Okay.

752
01:40:54.060 --> 01:40:54.300
Yeah.

753
01:41:02.430 --> 01:41:13.470
Yeah, so right. So the idea is you're building spark programs that you won't be able to run a standalone against a remote cluster or you want to be developing stuff locally that you would then

754
01:41:14.580 --> 01:41:24.090
Okay. Um, I meant to throw something together for tonight, but I didn't quite get it done. So what I'll do is next week we'll, we'll demo. Like, here's a basic Maven project or something.

755
01:41:24.390 --> 01:41:28.320
Stand up in Eclipse are intelligent and have a thing that runs locally and do stuff.

756
01:41:28.920 --> 01:41:36.120
And I probably can get done and even put it on canvas before next week. So if you want to pull it down and play with it sooner than that, that's an option.

757
01:41:37.110 --> 01:41:48.240
It's not super hard to do, but you know when you write the program, you basically have to start off, and you have to create a spark context and it'll get things configured and set some fields on it and there's sort of a, an incantation you basically do every time.

758
01:41:49.620 --> 01:41:52.020
So that's the question. Okay.

759
01:41:54.540 --> 01:42:01.380
Things are making noise. Alright, so let's go back to. Yeah. So let's, let's do a data center so

760
01:42:02.370 --> 01:42:10.140
So remember I said that slides bugging me either one of them says the data frames the other system. Oh, that's another inconsistently to watch data set.

761
01:42:10.650 --> 01:42:18.900
Especially if you're not working in our ID and you don't have a complete data set has only one capital letter D data frame has to capital letters. The D and E. F.

762
01:42:19.590 --> 01:42:26.010
So this is not, this is not pretty consistent. Right. Like, why isn't it data set and data frame said it's data.

763
01:42:26.520 --> 01:42:34.530
And data frame, right, like the capitalization isn't consistent. I think part of it was that maybe the art community capitalizes the D and E f a data frame.

764
01:42:34.890 --> 01:42:44.160
And the, the other thing is just know I think the real answers. They've done by different people and no one really cared about consistent. Okay, so let's get back into the

765
01:42:45.450 --> 01:42:47.520
ID, not the id the shelf.

766
01:42:49.380 --> 01:42:50.580
Start to show up again.

767
01:42:53.340 --> 01:43:03.570
It's not so bad when you're when you're in an ID with autocomplete but otherwise you'll find it like your fingers when once you get in the rhythm of typing one capital D capital F. They'll lapse into capital D capital S, you won't get anything

768
01:43:05.040 --> 01:43:18.240
Unless you do alias data set with a capital equal status check in every program you write in Scala which you can not saying you should be creating code that will puzzle. Other people when it doesn't work because they didn't do that. But you shouldn't have to do it.

769
01:43:19.740 --> 01:43:22.080
Okay, so now let's make a text Rd

770
01:43:23.340 --> 01:43:29.970
Equals se text file and let's go to our local file system data war and

771
01:43:31.560 --> 01:43:41.670
Text. Okay, great. All right. And then this. I can't read. You still have to do this or not. Looks like you. Well, if you don't have to do it. I did it.

772
01:43:42.180 --> 01:43:49.170
Okay, so a couple slides back. Remember I said we could create a data set data said capital A, capital S there. Oh well.

773
01:43:49.950 --> 01:44:00.240
But it's not an angel. And then so you spark read text file. You can also create them from a text file. So the top things just regular old already have strings.

774
01:44:00.750 --> 01:44:16.500
But this will be a different thing. So we're going to use the instead of using Spark context to read a text file into an RD. We're going to use the spark session to read a text file into a data set. And if that works. We'll get back a different data type there. It didn't work. Why

775
01:44:17.850 --> 01:44:24.480
Error was looking for motivated direct up because it didn't work. I typed it wrong. Alright, so let's fix that.

776
01:44:27.420 --> 01:44:43.050
No. Okay, so we got a different type here org Apache Spark sequel data set of strings. So that's interesting. So what can we do well, a lot of the stuff is the same as the stuff we did before to do text. Yes. Now line goes to lined up to lowercase.

777
01:44:45.570 --> 01:44:48.300
That will return a data set of strings strings.

778
01:44:49.320 --> 01:44:53.220
We can do. We're counting type stuff. So let's take

779
01:44:55.680 --> 01:45:03.750
Our texts that I call it yeah that map line goes to like split now. Does everybody remember that nap well enough to know what that does.

780
01:45:05.160 --> 01:45:06.330
Okay, so basically

781
01:45:07.470 --> 01:45:16.290
Yeah, right. So it basically takes an element turns it into a collection of things and then peels off the brackets. Exactly. Okay. So when you run that

782
01:45:20.100 --> 01:45:32.610
That's right, because you basically we want to split it on a reg ex, and we want to do white space and that slash has to be. That's the reg x's slam backslash so we need to back slashes to keep the backslash there.

783
01:45:37.980 --> 01:45:44.100
That would, that would have split on space, but it wouldn't have split on tabs and and other forms of, let's face it looks like I got

784
01:45:45.630 --> 01:45:51.060
Notified so words group words equal words group by key.

785
01:45:52.410 --> 01:45:55.920
And in this case, we can just, you know, take things to the thing

786
01:45:57.480 --> 01:46:02.910
OK, so now you see we've got a new type here. We've now got a key value group data set of pairs of strings.

787
01:46:03.930 --> 01:46:09.000
We have a key, which has got a string and value which is going to string and then if we do grouped words.

788
01:46:10.320 --> 01:46:13.260
Words count not shown

789
01:46:19.800 --> 01:46:30.030
Suspense. Oh, we see we see stages we see mapping and reducing okay so that's actually better than the kind of output we had before. Right. It actually came back and sort of nicely formatted table.

790
01:46:30.720 --> 01:46:41.430
Still appeared 600 times some of purity 25 times cold and comma on the pier 32 times because by splitting was distracted white spaces, rather than a punctuation, but get the idea.

791
01:46:42.480 --> 01:46:51.990
The word 07 probably didn't want you know, you know, what's your excuse me probably would have filtered off the off the trash and split up more sensibly, but you know this is a little bit better. Right.

792
01:46:52.920 --> 01:47:02.820
You know, the first thing just created a data set. The first thing on that slide. If we go back to go back to me in a second. It just created an RDS strings like we always right. So basically, every

793
01:47:03.180 --> 01:47:10.380
Every entry in the RD was a string, the corresponding to align in the file which is just a bunch of trash with commas in between it, which was kind of hard to to recommend

794
01:47:11.010 --> 01:47:20.550
And then if we worked on that we can convert it to the different types, but we were still doing things like you know we had the cat field and so on. Now we've actually got something where we've actually got

795
01:47:21.930 --> 01:47:34.650
You know, reasonable types of incidents, you know it figured this this group these things together. That's, that's a bit of an improvement, it could still be a little bit better. But, you know, we're getting closer to the sort of output. This sort of, you know,

796
01:47:36.180 --> 01:47:40.290
The sort of working your team that like real civilized databases of the past would have would have given

797
01:47:47.250 --> 01:47:54.960
Yeah. So basically what you're specifying the key should be. And so it's essentially just taking the first element that's there. If you look at the docks on group by key.

798
01:47:55.740 --> 01:48:00.210
It's sort of like doing reduced by key. But we're not, we're never going to reduce right

799
01:48:00.900 --> 01:48:11.130
So essentially what it's going to do with Roubaix P. We're going to group things together and it's going to basically do the task for us automatically right if you look at the dots on group, it will see it's just another. It's basically just another action.

800
01:48:13.440 --> 01:48:22.170
But it represents an operation that you actually do commonly enough that it's worth giving it its own name and its own separate thing. So now we can look at

801
01:48:23.490 --> 01:48:25.830
A slightly more real example right so

802
01:48:27.210 --> 01:48:31.290
Now, who's worked through the scala training notebook thing.

803
01:48:32.820 --> 01:48:35.820
Okay, does anybody remember the term case class.

804
01:48:39.120 --> 01:48:44.430
You recall seeing the term case class. Okay. Do you feel like you have a reasonable intuition. Forget what case classes.

805
01:48:48.030 --> 01:49:06.750
Yeah, we'll talk about them more in detail in a second. So basically, actually let's get the slide back up so that you can gaze upon this glory. While you're doing that toxin is just the first row of our airport data file. So the first row is the column names faces.

806
01:49:07.950 --> 01:49:15.090
And, oh no, actually, it's the first five rows. So the column names are the first line. The next four lines are the actual data. So in Scala.

807
01:49:15.870 --> 01:49:25.020
A case class is good for modeling mutable data once you create an instance of a case classic can't change. So say I had a case class called point that had coordinates index and the imply

808
01:49:25.500 --> 01:49:36.330
Then I could create point three, four and now I've got an instance of point three, four, where x and y are always three and four. They'll never change. If I want a new one. I could create a new one entirely I could copy that one.

809
01:49:36.660 --> 01:49:41.160
And then amended skills as I create the new one. But then that will never change. And, you know,

810
01:49:41.580 --> 01:49:49.020
Our DVDs arguable and RDP entries in place or readable, so you know it's natural that we'd want the data that we used to reflect them to be immutable so

811
01:49:49.320 --> 01:49:58.320
The first piece of pace classes is, you know, they're nice for modeling mutable or immutable data because they literally are intrinsically immutable in Scala, you know, if you want to make the class immutable in Java and

812
01:49:58.620 --> 01:50:10.140
Probably in Java, you should make everything mutable, you can always write unless you have a good reason not right because a lot of classes of bugs just disappear when someone can't change anything behind your back or you can't accidentally change.

813
01:50:11.460 --> 01:50:20.340
And, you know, it also when you look at Java employee to adopt some of the sort of functional programming style API's and lambda and stuff immutability this can work much more nicely to follow. So

814
01:50:20.910 --> 01:50:29.820
And this is actually another tip in another item, in effect, the Java is basically cleaver immutability wherever you can write it makes it makes your life, almost always easier so

815
01:50:30.450 --> 01:50:37.320
Beyond a case class and Scala being immutable. It also get some nice features, right, it automatically gets a method called apply

816
01:50:37.710 --> 01:50:41.970
Which in Scala, you almost never typed the name apply the method, what I'm thinking has it.

817
01:50:42.270 --> 01:50:53.250
Because scholar by syntactic sugar. If you type the name of the case class and perrins. It says, aha, someone's take the name of a case class with perrins that means he's trying to call its applied.

818
01:50:53.730 --> 01:50:59.190
And then it will take whatever you put in those friends and it'll call apply and pass those in. So the natural way to use that as a

819
01:50:59.700 --> 01:51:09.390
As a constructor. So if I did case class point and point heaven in Texas and New why I couldn't create a new one by saying point three, four, right, and then a new one would come with

820
01:51:09.900 --> 01:51:13.950
The X and the Y set two, three and four. We'll see examples of business seconds.

821
01:51:14.790 --> 01:51:27.210
The parameters the case class they naturally become public vowels. So if I gave that thing in name Val P equals point three, four, and then I said, p dot x, I get back three I said p dot why we get four

822
01:51:27.510 --> 01:51:35.970
And we think about Java right if I had private fields and you know you want to keep your fields as private as possible the time, you know. I'd have a private index and have a private why

823
01:51:36.570 --> 01:51:41.190
Maybe I make them final so they can't be changed after construction. So, then I've got my immutability

824
01:51:41.760 --> 01:51:50.250
And I've got my data hiding, but if I ever want to see those values outside the class, I have to add an access to, you know, public event yet X, nothing.

825
01:51:50.700 --> 01:52:02.340
Right, Richard next public event get why return. Why no, no markings right and you know that's that's boilerplate right like didn't need your, your classes. If you want to implement this kind of be

826
01:52:02.820 --> 01:52:10.050
Dinner center access for model, they start gathering these custom methods and Java will probably gain access methods at some point that that

827
01:52:10.740 --> 01:52:14.010
Similar to one C sharp has to save you having to write these methods yourself.

828
01:52:14.610 --> 01:52:24.870
Your ideas will generate right you can select a bunch of fields and say, generate the getters and setters and your lead 37 lines of junk for getting said, and all those things. But, you know, they still lit up the classes, a little bit so

829
01:52:26.010 --> 01:52:28.530
So what do we get in case classes. So far, we get this apply method.

830
01:52:29.700 --> 01:52:36.780
We get the parameters to the case class automatically becoming publicly visible families when we compare it to them.

831
01:52:37.350 --> 01:52:42.510
They get compared by structure, not by reference which is generally we want we're comparing values, right.

832
01:52:43.380 --> 01:52:51.900
In Java, it's possible to create a string, string s equals the quick brown fox and create a new string that also contains the characters. The quick brown fox.

833
01:52:52.260 --> 01:53:02.910
And call that TT. And if I do the comparison s equals equals t. I'm not guaranteed that will be true, right, those, those strings could be different objects referring to different things in memory, right.

834
01:53:03.420 --> 01:53:09.630
If they aren't, you know, it's either because I you know happens to just create a point or two points to the existing instance in the thing

835
01:53:09.870 --> 01:53:16.920
Or maybe in the library or the VM did something clever underneath to try to D duplicate them wish for something like strings. You could imagine it a try.

836
01:53:18.000 --> 01:53:24.480
But, you know, they don't have to be the same. Right. Um, you look at a class in Java, like capitalized integer, right, which is the box form of integer

837
01:53:25.530 --> 01:53:35.970
You know if basically created the integer zero which is used all the time, you know, and z equals new big integer zero or it's w equals new big interest zero again.

838
01:53:36.240 --> 01:53:49.800
The integer cast actually attaches the first 200 hundred and 27 or 28 or 256 integers because 03 127 03 256 or I can't remember what the exact boundaries. Those occur so often you don't want to be creating your object instances.

839
01:53:50.370 --> 01:53:56.220
So integer basically static we caches those right and it's staggering caches. Some other values and things like that. But, you know,

840
01:53:57.090 --> 01:54:04.020
The thing is you, you know, so those will happen to be equal for the small values, but for large values. Again, they wouldn't have to be right integer doesn't guarantee

841
01:54:05.250 --> 01:54:16.500
So in Scala, I could have feel, you know, Val P equals point three, four value equals point three, four again. And if I try to compare point three, four and you know p, q,

842
01:54:17.190 --> 01:54:24.150
If I'm comparing my value and ideally like that to be true. Right. And you know case classes are implemented with a comparison.

843
01:54:24.870 --> 01:54:31.380
Comparison mechanism that actually does do the right thing. It's a bit tricky because lots of things you see in Scala actually are Java References

844
01:54:31.740 --> 01:54:41.250
And you can still have other things that aren't case classes where you'll have all of your Object Comparison bugs that come back. But if you stick the case classes they almost always work the way you sort of naturally expect them to.

845
01:54:42.360 --> 01:54:52.620
They have a copy method, which is good for making shallow copies. So if you had point P equals point three, four, and you want it to be point three five, you could basically make a copy of it for change to

846
01:54:53.070 --> 01:54:59.910
The original 134 is still there, but the new 135 comes into existing and then maybe the most important thing is something called pattern matching

847
01:55:00.900 --> 01:55:14.310
Along with that apply method, there's a method called and apply that allows you to do a thing in Scala which you probably saw on the notebook called pattern matching, which is sort of like, you know, it's sort of like the case statement from C and Java grew up and became

848
01:55:15.480 --> 01:55:22.650
Right, and that you can pattern match based on structures. Right. And you can pattern match based on, you know, a variety of all training structures. Right.

849
01:55:23.280 --> 01:55:31.320
And we'll see examples of that as we go. So basically this this pattern matching thing to think of it, key statements that actually work the way you want them to be.

850
01:55:33.090 --> 01:55:38.400
So let's look at this loops back to the slide and see how it actually works. So

851
01:55:40.110 --> 01:55:50.670
Let's look at what's going on. So we had that slide, which just vanished from the screen, but whatever. So I'm going to type the case class definition out even though it has 47 fields, just to be pedantic about it.

852
01:55:50.940 --> 01:55:57.900
If you are writing a scholar program or if you're working in a notebook, you just define this in a cell evaluated and then the definition of the case class exists there.

853
01:55:58.440 --> 01:56:06.270
If you're working IE, you get all your nice autocomplete and syntax coloring, you'd be able to type it there, but I'm just going to type it based on the the

854
01:56:07.260 --> 01:56:23.250
Pre video file of the song so airports have two strings, and there was an identity string of category string and name string latitude agrees. I should take this correctly exactly with them for the duration of the session

855
01:56:24.510 --> 01:56:31.230
Longitude degrees, which was also, we want to be a double elevation feet, which was an integer.

856
01:56:32.730 --> 01:56:33.570
Continent.

857
01:56:35.100 --> 01:56:43.530
Which was a string if you catch me typing any of the English words in here incorrectly stopped me. So I don't kill again I so country string. I saw region.

858
01:56:45.090 --> 01:56:49.500
String unisex T string.

859
01:56:51.060 --> 01:56:52.860
GPS code string.

860
01:56:54.420 --> 01:56:55.440
I add our code.

861
01:56:57.720 --> 01:56:58.950
Local code.

862
01:57:01.980 --> 01:57:05.880
Okay, so if there are no typos. That should define the case last right

863
01:57:07.410 --> 01:57:13.290
So now I can do Val airports equals spark session read option.

864
01:57:14.340 --> 01:57:19.350
And I'm going to pass the option that's called inferred schema. I'm going to say make it true.

865
01:57:20.490 --> 01:57:24.360
I'm going to define another option called the header which will be set to true.

866
01:57:25.500 --> 01:57:29.070
And then I will read a CSV file which is

867
01:57:30.120 --> 01:57:36.600
The error port. It's underscore right didn't even look at Valerie. Can you tell me it's underscore. Oh well.

868
01:57:37.740 --> 01:57:46.380
As airport. Okay, so before I hit return on this. Let's walk through and deconstruct what this thing means

869
01:57:46.890 --> 01:57:57.270
And then I'll hit return. I think we'll see which parts but I got wrong because it'll fall apart. Okay, so what's this thing, dude. So regular scope of our Declaration I'm defining a value called airports.

870
01:57:57.570 --> 01:58:02.100
And telling the spark session to read something and then this is kind of a fluent builder up

871
01:58:02.880 --> 01:58:10.860
I've got this thing. And I'm going to return sets and properties are set some parameters and then call to action on down the road. So one of the options is called first schema. True.

872
01:58:11.370 --> 01:58:19.710
The other option which goes with that is header. True. So in first scheme of things, when you read the file, try to guess the types of these things and file or

873
01:58:19.950 --> 01:58:24.330
And match them using reflection against the class that I'm trying to tell you to stop this stuff.

874
01:58:24.960 --> 01:58:34.950
Right. So when you saw that file that file control. We had a thing that looked like a string for the identifier thing. It will take a string to the category thing that would look like a string for the name and get a couple things that look like doubles.

875
01:58:36.330 --> 01:58:41.580
And so in French schema says well as you parse this thing, trying to turn these things into these types, right.

876
01:58:42.180 --> 01:58:46.110
We'll use the header line in the file. We had a true he's looking at the first time.

877
01:58:46.440 --> 01:58:54.750
The file and take it to me the names fields, right, and then CSV said this. Okay. The Bible is a CSV file deal and then this as airport means

878
01:58:55.110 --> 01:59:02.430
When you read the file, all the things you can pull out of it, according to those things we asked for trying to turn them into the instances airport case.

879
01:59:02.940 --> 01:59:10.590
Right, you look at them crack up the fields, one, one. Try to course the types from the strings that are coming out of the file. And, you know, if it's a string. You can turn into a double what it says.

880
01:59:10.860 --> 01:59:19.890
Police actions trying to turn the old stuff in there, right. So, assuming I define the case class correctly and define this wrote this line correctly.

881
01:59:21.180 --> 01:59:22.380
something should happen there.

882
01:59:23.460 --> 01:59:35.610
That we saw something happened. I created a data set of airports, right. See the type rabbit in your org Apache Spark sequel data set square brackets in this context and scalability type parameter. So that's a data set of airports.

883
01:59:35.880 --> 01:59:43.950
And then there's a sneak preview of what these look like well identifier string category stream. Well, let's see. That's what we want them to be, of course, like maybe they're all

884
01:59:45.030 --> 01:59:50.100
A lighted the next 11 maybe it just, just make them all string and it blew me off, but we'll see in a minute, but it didn't.

885
01:59:51.240 --> 02:00:06.120
So that's cool. So now if I take this airports data set and do show 10 right. Show me the first 10, what do I get right well something too big to fit the window. So it looks like crap. But, you know, if I make it a little smaller.

886
02:00:07.590 --> 02:00:08.730
And a little bigger.

887
02:00:10.650 --> 02:00:13.590
Smaller bigger

888
02:00:14.700 --> 02:00:18.000
Okay, that's actually okay that's half respectable so

889
02:00:19.800 --> 02:00:25.710
What did it do. Okay. That's pretty good. We got a nice table here we've got names for the columns names for the fields.

890
02:00:26.550 --> 02:00:35.670
The things. Well, you know, those, those, those coordinates could still be streets. We don't know for sure, from what I printed it might just literally put quotation marks and put a bunch of numbers and the period.

891
02:00:36.120 --> 02:00:41.610
Decimal and negative sign stuff in there that does your car cane, a little bit. But beyond that, that looks pretty incredible. Right.

892
02:00:42.840 --> 02:00:46.260
And that's nice. But what do we get from it. Next.

893
02:00:49.050 --> 02:00:50.280
Well, let's find out.

894
02:00:51.840 --> 02:01:00.360
First off, did everybody see what we did in that exercise and why there was anything new there or did it just looked like, you know, yet another Rd with like slightly different syntax for creating it.

895
02:01:02.400 --> 02:01:03.270
Almost all

896
02:01:12.090 --> 02:01:12.450
That

897
02:01:13.860 --> 02:01:17.490
Yeah, so you want to get back to the definition of what

898
02:01:21.810 --> 02:01:22.050
That

899
02:01:23.280 --> 02:01:23.700
Means

900
02:01:26.070 --> 02:01:37.470
When I said I'm the. Yes. So the options I past what's going to try to do is it's going to try to take you. It's going to use reflection to take the names of the airport class what I said as airport

901
02:01:37.680 --> 02:01:51.270
It can use reflection to look at the definition of airport and figure out what those names should be and then I will try to match them against the corresponding things for the headers. So if I had to use it. You know, I was getting money from different regions.

902
02:01:52.350 --> 02:01:54.300
Right, yeah. And one of them coffee.

903
02:01:55.920 --> 02:01:57.960
Yeah, you might you might end up missing you

904
02:01:59.760 --> 02:02:00.180
See

905
02:02:02.610 --> 02:02:10.950
A natural thing to do would be to transform the data before it comes in right so you can take if you knew this one column was just badly named right you you know you could you could basically transform that call.

906
02:02:11.130 --> 02:02:22.500
Or convert the thing. Afterwards, I think there's also an option where. So this is where we infer schema. You can also explicitly define the schema where you could basically say, I want this thing to be one of these and to go here.

907
02:02:23.160 --> 02:02:38.730
But with the first name is kind of nice because if your data is complete on the structure nebulous trash, but it at least has enough regularity. Then, and this gets it right then, then you've gotten pretty far right. So let's take an example here. So let's do first.

908
02:02:39.810 --> 02:02:40.710
And let's take

909
02:02:41.910 --> 02:02:45.090
The first 10 airports golfers 10

910
02:02:46.680 --> 02:02:53.640
Okay, so this gives back and array of airports, I'll make this bigger so you can see it, since we don't need to see the whole table anymore. That's too big.

911
02:02:55.350 --> 02:02:56.040
And now it's too wide.

912
02:02:57.690 --> 02:03:04.920
And it's like, freaking three bears here everything is like warm and cold. To make. Okay, so there's an array of airports.

913
02:03:06.030 --> 02:03:12.600
It subsides 10, which is what we expected. That's what we asked for. If I asked for airport zero

914
02:03:15.810 --> 02:03:16.680
First 10 zero

915
02:03:18.900 --> 02:03:32.850
Is the first 10 zero, I get back an instance of this case class airport. Right. So I can say val AP one equals first 10 zero validate the two equals first

916
02:03:33.810 --> 02:03:41.490
One. Right. And those are actual, you know, bona fides calculate static place I could go write a stellar program. They took these things out.

917
02:03:41.910 --> 02:03:49.740
Did pattern matching on them, you know, manipulated them like first class real style objects with strong typing and an older properties case classes have

918
02:03:50.040 --> 02:03:54.840
You have to work with case classes. Little bit distorted appreciating them, but once you get the hang of writing pattern matching

919
02:03:55.140 --> 02:04:01.560
Like you're creating every time you have to go back to the language doesn't have it, like if you use pattern matching and Ross Haskell or Scala.

920
02:04:01.800 --> 02:04:09.810
And then you have to go back to other languages that have just a case statement or worse like or what if LC LC LC that you know you'll just cringe, because

921
02:04:11.250 --> 02:04:19.500
He's classes can also match on incomplete patterns were like, you might say, hey, I wanted to match all see that going back to our example we have points that have x and y. So,

922
02:04:19.710 --> 02:04:23.910
I could do, you know, sort of like the station, I could say case three comma underscore

923
02:04:24.240 --> 02:04:31.110
Go to do some thing else right and that means I'm only matching against points that happened to have x equals three. I don't care about why

924
02:04:31.410 --> 02:04:42.780
Or I could do case point underscore five I'm only matching against points that have y equal to five. I don't care about X right or you know match point underscore, underscore, I don't care about it. Right.

925
02:04:43.590 --> 02:04:45.480
And, you know, think of the way you'd have to do it and switch things

926
02:04:45.750 --> 02:04:53.970
Well, first off, you could write because you can't put arbitrary object types in the cases of the switch statement. In fact, you couldn't put strings in them until very recently in Java.

927
02:04:54.300 --> 02:05:07.530
Right, if you had a bunch of strings. You want to compare your best about putting them in a sandwich or some other collection and working with them that way like strings and switches were actually those even in I think they're eight or nine strings in Java switch statements.

928
02:05:08.610 --> 02:05:14.220
They've been talking about putting him in for 10 years. I think they're there now but I couldn't tell you off top my head which version that actually came out in

929
02:05:15.510 --> 02:05:22.290
And will mess with some pattern matching examples where you can see it later. But the point is he taking this data from like this. This machine nebulous text.

930
02:05:23.430 --> 02:05:35.760
To columns had names and it was kind of clear to grants, what we'd like to taste it. But we basically wrote the scholar cost case class, the corresponding to what that raw text lies and really doing nothing after we can find the case class, other than telling

931
02:05:36.630 --> 02:05:41.520
The data sets API had read it, it gave us back a collection of nice Scala case classes.

932
02:05:41.790 --> 02:05:54.720
Which we give them right like a grown up big boys Scholar Program with with pattern matching and everything else which is much less much more pleasant. Right. So I can take, for example, remember I said there you've got exit answers for for free.

933
02:05:56.640 --> 02:06:00.210
So let's see, at one dot i said country.

934
02:06:01.980 --> 02:06:16.410
Us right AP to dot latitude degrees and see it auto completed because this is actually, this isn't just me guessing that it's the fourth column or the fifth column or whatever. That's an instance of a scholar class of tight airport

935
02:06:17.280 --> 02:06:23.640
And you know, it's got fields that the interpreter knows about. So I can say latitude degrees. And look, I got back a nice civilized world we had double

936
02:06:24.060 --> 02:06:32.250
Right, that's much better than you know line goes to like split comma seven to double. I hope seven was right.

937
02:06:32.640 --> 02:06:42.330
You know, you know, it might have been almost right. Maybe it was a different couple and it would still give me an answer here. At least you know I've got an honesty. If that the initial conversion got through this thing isn't total trash.

938
02:06:43.440 --> 02:06:48.660
So that's, that's the fields becoming public vowels. If I want to compare them by structure.

939
02:06:50.760 --> 02:06:55.560
That doesn't necessarily prove very much because these things are clear the different, but I could also make a copy of one

940
02:06:56.280 --> 02:07:03.300
And you know, I can make a copy of AP one which we actually did for an object and then I could compare them, and I would actually get that they really are the same.

941
02:07:03.750 --> 02:07:12.300
Because case classes are intended to be treated as values. So when you do the comparison. It gets overloaded to actually call the appropriate method to do a shallow compare on the contents, rather than

942
02:07:12.600 --> 02:07:21.390
Are these things both pointing at the same thing and memory, right, as opposed to pointing at two things in different places and memory that happened to be locally identical, but you know, I'm not gonna look that custom

943
02:07:23.220 --> 02:07:24.840
Okay, you

944
02:07:30.060 --> 02:07:38.250
Know if it can literally create a new copy. Right. But when you compare them like there'll be compared. For like logical equality for pointer.

945
02:07:38.970 --> 02:07:48.300
Right, and that's the tricky. One thing you want to avoid. This is the people get this wrong with Java programs and strings all the time, right, is that they have two strings that if you called. You know, first string.

946
02:07:48.660 --> 02:07:56.760
Equals other string you get true, but if you said first screen equals equals other stream you get false because there are two different objects and memory.

947
02:07:57.000 --> 02:08:05.100
They happen to have the same content and mean the same thing as strings, but the Equal. Equal. Equal symbol doesn't compare that just checks to see if they're in the same place.

948
02:08:06.300 --> 02:08:10.920
And that's a sort of weaker condition that that that that notion of equality.

949
02:08:11.460 --> 02:08:30.600
Has haunted programming languages, since the beginning like Lisp had multiple notions of equality, there is EQ and equal or very various versions or ETV and things like that and languages that are unclear about it are really hard to programming because tree question. Okay. Um, so

950
02:08:32.040 --> 02:08:41.430
Yeah, so you'll see you'll see more examples of case classes as you go and you know uses up we can come back to them. I don't know. It's going to sound like I'm

951
02:08:42.240 --> 02:08:44.400
A preacher evangelizing case classes but

952
02:08:44.940 --> 02:08:46.620
The pattern matching alone makes the good

953
02:08:46.920 --> 02:08:56.370
Right, like once you use patterns change language that has it languages that don't have it, it's, it's like wearing someone else's shoes and they're the wrong size and then we can have holes in them and and their shoes are

954
02:08:56.760 --> 02:09:01.920
Horrible. And they probably did you no favors by letting you steal their shoes, but you get

955
02:09:03.360 --> 02:09:04.830
Okay, so

956
02:09:06.480 --> 02:09:19.680
Yeah. So now let's see, we want to do some other things. So remember these data sets that we get back once they're created they contain actual Scala optics web, you know, was case classes a particular type of Scala, so we can do other stuff so

957
02:09:20.070 --> 02:09:24.330
Let's look at here. So we have airports when they call in airports.

958
02:09:25.470 --> 02:09:43.050
And that was the airport's airports. Good. Okay, so let's do Val airports California equals airports dot. And then we have all our old sort of Sparky MapReduce things here. So filter. Right. And so remember how we did this before, right.

959
02:09:44.190 --> 02:09:59.430
The airport stop filter line goes to line split comma perrins whichever field contained the state. And then we had to compare that to California or whatever, right, so here's what we're going to do here.

960
02:10:00.240 --> 02:10:04.380
Remember what I said about the underscore when there's only one argument. So it doesn't really matter. So

961
02:10:04.740 --> 02:10:13.050
Whatever that field was called it was, you know, we could say he goes to a call some method, but because there's just one dummy parameters, no point in getting a name.

962
02:10:13.620 --> 02:10:26.970
ISO region by country. I said, we see I had complete there and it showed me both the things that started with ice. So if I want, I still region that and then I can do the string completely

963
02:10:28.290 --> 02:10:40.170
You know, because strings are not defined are treated as a immutable value class here. So this is pretty nice. Right. I didn't have to you know line goes to like split comma seven

964
02:10:41.610 --> 02:10:46.890
String so we can have to do anything more with it. But then we do dot equals parabens us.ca

965
02:10:47.340 --> 02:10:55.380
Job right here. I basically just use what looks like a regular equality operator later on we'll see a triple equal sign for different thing. But, you know, it'll make sense.

966
02:10:56.220 --> 02:11:05.580
And when you get it wrong at least get an error message that gives you something wrong, but that that's jumping ahead. So when we hit that wouldn't get it. Well, we get a new data set.

967
02:11:06.990 --> 02:11:08.250
How many of them are there.

968
02:11:12.630 --> 02:11:22.020
1016 and airports head what 21,000 46,000 and that's the same number of things that we found at the California airports example last time using already these maps right but

969
02:11:22.260 --> 02:11:35.340
Like who thinks this is actually more humane, then the language line split karma. Whenever field number. It was study equals four. Right. You know, when we get there, we got like auto completion of the names

970
02:11:36.000 --> 02:11:38.430
You know, we couldn't come up with one that's just wrong, you know,

971
02:11:38.640 --> 02:11:47.340
It actually has a name, so we can tell which one we're looking at. We got a nice comparison as quality, we need to be fair Scala games for free. In the previous example. But if you were called spark job and you would get that

972
02:11:47.520 --> 02:11:56.730
You've had to call you know strain on equals on nothing in order to get it right. Because there'd be no guarantee, you'd be correct. So that's already better

973
02:11:58.500 --> 02:12:08.670
And similarly, we can do, what did we do the reports in California. So we can take the airport stock filter and I so region.

974
02:12:09.810 --> 02:12:18.570
Again equals was a US CA and we can add another filter transformation to downstream and there was a field called category.

975
02:12:20.340 --> 02:12:24.960
So FCA O Kenny or category categories. Good.

976
02:12:26.160 --> 02:12:27.900
Equals teleport

977
02:12:29.670 --> 02:12:32.460
Filter save you on all the ones that are at some high elevation

978
02:12:34.050 --> 02:12:37.920
Elevation feet greater than 1000 get return

979
02:12:41.580 --> 02:12:46.650
And look at the first five. And there we go, there's a paper. One of the things you

980
02:12:47.730 --> 02:13:01.050
Will see they're all about 1000 or all the California, but you know this was certainly less awful than what the raw Rd things look like. Right. We didn't have names. We had to call them positions. We didn't have meaningful type checking

981
02:13:01.980 --> 02:13:10.890
You know, without the names. We couldn't easily use the the underscore syntax. In some cases, because there might have been another thing along with it.

982
02:13:11.490 --> 02:13:18.780
So that that's that's a little bit better and the tab completion alone if you're working with interactively even working with an ID. It's nice to have a class.

983
02:13:19.020 --> 02:13:31.650
THAT ACTUALLY HAS AN ADMITTED date on it that the compiler and the ID and the shell all know that if I type you know airport dot ISO. Well, there's an actual region racial country. It's got to be one of the two and it can offer them to write

984
02:13:33.060 --> 02:13:35.700
And again, it's able to do that sort of by reflection.

985
02:13:36.900 --> 02:13:42.570
So that's definitely better than the RD streaming thing where we had to guess column positions and all that stuff.

986
02:13:44.730 --> 02:13:58.350
Okay, so let's get back to the slides. Does everyone did. Everyone follow that. So the takeaway pieces on that are the things to remember what is the case class. And why is the case class. Good.

987
02:13:59.610 --> 02:14:06.930
How, what is the data set, right, a data set, basically, is a thing where you couldn't create a thing that basically has an RV underneath.

988
02:14:07.230 --> 02:14:16.530
Many of the RTB operations, but it's going to be represented as a logically an Rd of instances of whatever today's class you told it to try to turn into

989
02:14:16.860 --> 02:14:23.850
And the, the spark session has ways of reading things like CSV files in with a few days convenience functions like you know

990
02:14:24.240 --> 02:14:32.910
Take the names matter fields in for the schema when you can and stuff like that and actually mapping it up and matching up with the case class you created to accommodate this stuff on a glance.

991
02:14:33.630 --> 02:14:38.190
And that gives you stuff with much, you know, you kind of go guardrails right now you've got, you know,

992
02:14:39.060 --> 02:14:44.250
instances of classes that have fields with names and types that you can check that the ID can Autocomplete for you.

993
02:14:45.000 --> 02:14:56.190
You know that you can use in Scala pattern matching in the constructs like that right civilized programs as opposed to all this junk where you really are just starting out strings here and there and hoping you get the right ones, and you can turn them into the right things.

994
02:14:57.090 --> 02:14:57.930
Any questions about that.

995
02:15:00.180 --> 02:15:00.900
Senses

996
02:15:02.040 --> 02:15:03.810
Have to work with actual already

997
02:15:05.220 --> 02:15:06.270
effectual like a

998
02:15:10.140 --> 02:15:19.320
Lot of cases where you know if your data is so trashing the promise you if your data is really, really trashy. There might be almost no way to work with it isn't already di that without some cleanup right

999
02:15:19.890 --> 02:15:26.370
But in general, like if you if you like doing sort of our style data for any stuff using the data frames is probably going to be comfortable and happy for

1000
02:15:27.210 --> 02:15:36.150
If you plan to write Scala programs around these things and take advantage of some of the idioms and syntax and flexibility and features and power Scala.

1001
02:15:36.420 --> 02:15:45.390
Then data sets or a supernatural thing to us right because they actually match against the scala object model style type system and a lot of these sort of syntactic conveniences in ways that

1002
02:15:45.780 --> 02:15:52.200
These, these sorts of benefits. So it sort of comes down to how do you feel comfortable working with the stuff. Once you pull it in

1003
02:15:52.860 --> 02:16:06.750
And then from this format started in. Did you have an easy way to pull it into that formula comfortable working right. And in the case of data sets, you know, you saw you define a case class that's the right shape for the stuff land in and then you use read dot

1004
02:16:07.770 --> 02:16:23.280
TXT file CSV, possibly with the options said suitably to do the things you want and then you can get it straight in there and almost one shot, you know, unless it's really malformed, so if you had like a plan like what are you just wanna. Yeah. Usually we say

1005
02:16:24.690 --> 02:16:31.800
Well, what would the case class be represented. But you could say maybe maybe paragraph, like, but, you know, but then again paragraphs are different length.

1006
02:16:32.100 --> 02:16:41.370
The paragraphs are different links, right. So, like, how would you name the fields of the paragraph, right, like you, who would even know what the right number of them to have would be so in that case the data may not be structured enough

1007
02:16:41.700 --> 02:16:50.220
Right. But for a lot of sort of our database you things like our home values or airport values or a lot of sensor data and stuff like that. It's a much more natural, natural right

1008
02:16:50.970 --> 02:16:57.720
You know that that's one place where the MapReduce model is actually kind of good is if you have to do large scalable parallel distributed whenever

1009
02:16:57.900 --> 02:17:04.440
On data, the GIS kind of trash like you know all the just random piles of text on my pages are all the URLs and the pages for

1010
02:17:04.950 --> 02:17:12.960
Things like that, then you know MapReduce in whether it's in Spark or the new version is kind of low level and primitive enough you can kind of better this stuff into whatever shape you want.

1011
02:17:13.710 --> 02:17:22.560
But if this stuff is already close to having a shape. These are ways to basically jumping from that shape into that shape in a container that actually helps you sentence.

1012
02:17:23.880 --> 02:17:24.150
Okay.

1013
02:17:25.350 --> 02:17:34.380
So with that, we'll move on from data sets to data frames or we can move them to the summit's today sets and maybe we're just kidding. There, there was just a typo in that silent.

1014
02:17:35.370 --> 02:17:47.610
Spring sermon data sets. I think I switched over and start to datasets almost straight away. But if you were confused by the fact that, like coming up next data frames and then coming up next was other crap entirely that was totally legitimate reasonable

1015
02:17:48.150 --> 02:17:53.880
OK, so the jump back to our cartoon picture this thing on the right is sort of what we had with

1016
02:17:55.830 --> 02:18:06.120
A sense. Right. You know, basically, we had a collection of these things, these orange boxes represent Scott Hayes classes, the things here basically are the the

1017
02:18:06.930 --> 02:18:14.910
The instance fields of those Scala case classes which because their case classes naturally become public members, you could just pick off and and access however you want.

1018
02:18:16.230 --> 02:18:18.180
The thing on the left is more sort of the

1019
02:18:19.080 --> 02:18:31.860
Or the data frames and the term data frame here. Basically, it's the way data frame is used roughly in our it's the way data frame is roughly used in in the Python libraries that deal with data frames and in many ways there's similar to our DB tables.

1020
02:18:32.700 --> 02:18:41.610
You know, and it's structurally and sort of behaviour, although you can't necessarily do all the cool stuff you can with them in their DNS. So the difference

1021
02:18:43.530 --> 02:18:50.610
See, well that that wasn't that was that was auto complete like assuming that you never want a capital letter in the middle of a word.

1022
02:18:51.000 --> 02:18:59.280
It's really sucks when you work for a place with a name like VMware capital D capital and small w AR D is you can't type that into anything come up. Correct. Right.

1023
02:19:00.360 --> 02:19:03.300
So the trick is you should get spun off into mushy your child.

1024
02:19:05.220 --> 02:19:08.340
With a name that's a natural English words. So it will automatically be capitalized.

1025
02:19:10.110 --> 02:19:11.670
No I don't suggest that as a solution.

1026
02:19:12.750 --> 02:19:13.110
But

1027
02:19:15.810 --> 02:19:32.310
Haven't got that job okay so data frames capital D capital. So that first line of the bone is wrong. So they're distributed data collectors. Again, they sort of are organized and domain columns, like the data frames, you might be familiar with from our or the suitable Python packages.

1028
02:19:33.360 --> 02:19:33.810
And

1029
02:19:34.890 --> 02:19:40.650
We can construct them like we can't data sets are many ways files or high tables or external databases or just the RDS

1030
02:19:41.070 --> 02:19:47.370
It's probably not a big surprise. You can easily make them from hive tables external databases, because those things are already collections of rows of

1031
02:19:47.640 --> 02:19:53.250
Named types columns as you're just going to create another thing which is a collection of rows of names type columns that

1032
02:19:53.670 --> 02:19:59.010
Like if you can't convert one of those things into the other things with some reason like you probably don't have one of those things.

1033
02:19:59.820 --> 02:20:03.510
So let's actually see how these work. I'm also, this is the thing to note

1034
02:20:04.410 --> 02:20:09.630
Python, at least the last time I looked, I believe this is still true, the Python bindings for spark don't support datasets yet.

1035
02:20:09.960 --> 02:20:19.110
You can only use Spark SQL data sets from Java or Scala, and they're less nice from Java because you don't have case classes and you have to do with a couple other things.

1036
02:20:19.560 --> 02:20:29.580
And you won't necessarily have the equals equals behave the way you want it to on something. So you'll have to remember to explicitly call dot equals on things and stuff like that so

1037
02:20:31.710 --> 02:20:40.020
Warning goes here spark sequel is in general a spot where Python support and spark is kind of lagging right. And part of the problem is that

1038
02:20:40.560 --> 02:20:51.180
Python doesn't have this sort of type system Scala does, right. So a lot of the stuff Scala can do with its intrinsic type system or with the ability to use the JVM reflection stuff Python doesn't quite have

1039
02:20:51.660 --> 02:21:01.020
Right, so there are cases where Python might have to make a best guess or where you might have to sort of nudge it along. And so it hasn't been an actual easy went off to just kind of port that functionality.

1040
02:21:02.790 --> 02:21:11.730
That's one problem with wrapping things from different languages is that if a thing started life in a language that contains ideas that the language you want to access it from doesn't even have

1041
02:21:12.120 --> 02:21:27.120
How to express those is great, you know it's it's it's you know it's you know human languages. Generally, one hopes they can all say roughly the same things and be translated to one another, but it's not always obvious, right, especially in more idiomatic parts of the language.

1042
02:21:28.440 --> 02:21:30.120
The yeah

1043
02:21:31.710 --> 02:21:42.090
Idioms often don't translate. But imagine you had languages where certain ideas couldn't even be expressed in some ways, right. And so if you met someone a traveler from another land and tried to explain all this stuff that you knew about you like

1044
02:21:42.540 --> 02:21:48.750
My language doesn't even have words to be talking about that. Right. You know, and then you get into the, what do you break it down to

1045
02:21:49.290 --> 02:21:57.720
There's a funny recording the talk I probably put in the readings for this week. Related to that, but anyway. So let's actually start looking at data frames in what they do so.

1046
02:21:58.440 --> 02:22:13.380
We'll do the sort of same sort of stuff we did with data sets a few minutes ago. But we'll do them with data frames. So again, we got our case class airport. Let us pray that it still works. So let's again to airports.

1047
02:22:14.430 --> 02:22:26.490
But let's pull them into a data frames. This time, instead of the data set. So let's make an airports def for data frame spark read option. Again, let's tell it to infer schema.

1048
02:22:31.980 --> 02:22:38.100
This is a little bit nasty these these names string fields because basically it's building its building a hash map internally.

1049
02:22:38.610 --> 02:22:45.600
Of names of options devalues of options and they're all strings. So there's no typing error. The upside is if you make up an option is totally wrong like headers.

1050
02:22:46.170 --> 02:22:55.500
It will probably throw an error saying you're trying to specify an invalid option so it's it's not as fragile as it looks, but it's close. Okay. So then let's get file.

1051
02:22:59.640 --> 02:23:02.910
Look at the docks. Again, it's not like you can't even really autocomplete in

1052
02:23:03.360 --> 02:23:09.180
It right because it's just a strange, they're going to be looked up and some internal hash table. And that might differ from version to version.

1053
02:23:09.420 --> 02:23:19.770
It might even differ depending on how the things configured. Right. You can imagine extension mechanisms that would add options and like sort of mix them in later, in which case you know the front, you wouldn't know and all that.

1054
02:23:20.880 --> 02:23:28.620
So sadly not really okay so similar to before spark read option in first game, a true option hitter true CSV that stuff.

1055
02:23:29.760 --> 02:23:39.030
As airport. So that's our coercion saying we're going to pull that stuff in. We're going to take names from the first row we're going to try to guess the types of these things from what we see.

1056
02:23:40.050 --> 02:23:49.830
And then we're going to try stuff in this case classes. And then if instead of to data set I do to DEF, and if I take it right then stuff will happen.

1057
02:23:51.180 --> 02:23:54.420
And then if I try to show this thing show the first five

1058
02:23:56.190 --> 02:24:06.090
Well, look, it's incredibly different from what we saw before. I mean, the differences just leap off the page. I know exactly, exactly the same output. So

1059
02:24:07.170 --> 02:24:16.290
What's interesting about that you get a little bit of stronger typing with data sets that you didn't quite have before. So the

1060
02:24:17.520 --> 02:24:17.880
Year.

1061
02:24:22.620 --> 02:24:37.710
Yeah. Sorry, I lost my train of thought. Um, yeah. And so we'll see. So let's. We'll see, we'll see the typing in a second. Sorry. I looked at this screen and then I got turned around. OK, so now you can do a couple things that are a little bit different here. So let's try

1062
02:24:39.570 --> 02:24:39.930
Yeah.

1063
02:24:46.710 --> 02:24:51.000
Yeah, oh, let's try. Let's go back to the line.

1064
02:24:53.220 --> 02:24:57.630
This was the only one here. And so we're just gonna get rid of the type conversions as

1065
02:25:00.540 --> 02:25:12.180
Well, you got a data frame right but you'll notice a data frame of what right and if you look at the airports phone I just caught with my original airports DF

1066
02:25:12.570 --> 02:25:21.540
Let's call this one airports def, blah, blah, whatever it was. And then let's go back in and get the original airports DF again. So this is the one with the airport.

1067
02:25:22.530 --> 02:25:32.910
And so this is a good question. So contrast the difference. They actually looked the same up to that level. But if I take the whole week so airports def. Wow.

1068
02:25:34.170 --> 02:25:39.450
Take five. So let's just take the first five years old and I have an extra period in there and

1069
02:25:40.830 --> 02:25:45.000
Take that wouldn't do I got right so I've got

1070
02:25:46.260 --> 02:25:49.500
I've got an array org Apache Spark sequel row.

1071
02:25:50.280 --> 02:26:00.930
Right, because without knowing to take those roads because ultimately, and this is a good question because it illustrates what's underneath right we ultimately got an Rd have these things called roads.

1072
02:26:01.470 --> 02:26:11.640
Org Apache Spark sequel roads, and in the case of as airport, we were doing enough of a hint to say take the fields in those rows and match them into this airport case class.

1073
02:26:11.850 --> 02:26:18.840
When we didn't ask for that. It just basically gave us back the raw underlying thing that's there. So if I do airport Steve wah wah tape.

1074
02:26:19.470 --> 02:26:26.730
Let's go stuff let's let's grab the first five rows. And then if I take the first one I get back this row object.

1075
02:26:27.660 --> 02:26:36.090
Well, I got that. But like, you know, what am I going to be with him. Right. I don't have the they make the case class and properties anymore because it just didn't know what to turn them into

1076
02:26:36.690 --> 02:26:45.720
Make sense. So the short answer there is that the underlying structure is really an Rd like collection of these things called sparks equal rose

1077
02:26:46.410 --> 02:26:55.980
But when we do the as we're basically saying, Okay, take those things and try to coerce them into this case class because we ultimately would like to work with these things wrapped and treated as case classes.

1078
02:26:56.610 --> 02:27:03.120
But if we don't tell it that then the most it can do is just give us back the spring up rose, which you know won't give us all the nice property simply won't

1079
02:27:04.860 --> 02:27:06.060
Make sense. OK.

1080
02:27:08.280 --> 02:27:12.720
OK, so now let's get back to whatever I called that most recent thing. What was it

1081
02:27:14.340 --> 02:27:28.710
Airports def yeah okay so that seems plausible. OK, so now let's see, we want to get the California airports again so airport ca from the data frame airports, the F dot filter.

1082
02:27:29.670 --> 02:27:37.110
Okay so filter. We're going to get some kind of predicate. But what's it going to be right. So ideally, we know that the

1083
02:27:37.560 --> 02:27:42.990
columns in the data frame have names right so we need some way to basically specify the name of a column and compare it to something

1084
02:27:43.410 --> 02:27:56.010
So let's go take. So the trick is this thing called the dollar sign and dollar sign takes a string and converts it to a column name and then once we have a column name columns have this triple equals to find out.

1085
02:27:57.570 --> 02:28:11.790
Which allows us to you. So basically the thing on the left, takes this through I sorry converts it to a column name if it can and then the triple equals tries to match the contents of the data in that column with us, CA and if it works.

1086
02:28:12.900 --> 02:28:18.840
Let's see if we get our 1016 year, whatever it was like before. Well, there's the same number of entries. So obviously the same pieces.

1087
02:28:20.460 --> 02:28:26.940
Know they they are interesting but but the trick. There is the things that are different now is that

1088
02:28:27.810 --> 02:28:36.390
The filter thing because we now have these named columns to work with, as opposed to got like case class instances to work with. We have to basically take

1089
02:28:36.960 --> 02:28:42.960
The name of some column and figure out where does that column actually equal some particular container particular value.

1090
02:28:43.320 --> 02:28:53.670
And so for the equality test is triple equals and to convert today. And so ways, you'll get this wrong. If you forget. So suppose we forget the dollar sign. Anyone want to guess what it really

1091
02:28:58.290 --> 02:28:59.190
Takes to get that far.

1092
02:29:02.160 --> 02:29:09.960
Yeah, so there's, there is value triple equals, it's not a member, a string because triple equals Scala has operator overloading.

1093
02:29:10.230 --> 02:29:20.850
And when you see an operator like this. It's really a method call on the thing to the left today. So to go back to this. I could put these watch me prove myself wrong. I couldn't put a.on there and

1094
02:29:24.330 --> 02:29:25.920
Oh yeah, then I'd have to put

1095
02:29:27.120 --> 02:29:28.440
parentheses around this.

1096
02:29:30.210 --> 02:29:31.560
Mentioned on the show complete

1097
02:29:32.580 --> 02:29:47.130
Yeah, great. Um, so this is how Scala operate only works when an operator is overloaded on right it's a method, whose name is those symbols and Scala allows you to call a method without the dots. So if I had

1098
02:29:47.670 --> 02:29:57.390
You know, RD dot map whatever I can also add space map whatever right Scala allows both of them is alternate forms. Sometimes it's more readable.

1099
02:29:58.950 --> 02:30:03.390
You will sometimes see the mixed in a given Scholar Program. And there are certain cases were

1100
02:30:03.810 --> 02:30:10.680
Leaving out the dots and going with spaces makes it more like English and there are certain cases where if it starts three too much like English, it becomes hell.

1101
02:30:11.100 --> 02:30:15.180
Because think about how many crappy ways there are to express any given idea in English.

1102
02:30:15.450 --> 02:30:23.910
Right now imagine you've got an English like programming language. We're only a subset of all the crappy ways you could have said something English or even sentences.

1103
02:30:24.150 --> 02:30:32.610
Right. So you think with some here. I mean, this is an example to a whiteboard and show you the sorts of errors, you'll get what you get it wrong. So if I forget the dollar sign

1104
02:30:33.300 --> 02:30:43.320
It seems like a region as a string it tries to find a method called triple equals on ISO region and there isn't one. And so I get the complaint triple equals is not a member of string when can't do anything.

1105
02:30:44.610 --> 02:30:56.850
If I then go here and say dollar quote I sorry and that whole thing. So each and then becomes a an instance of column. And in fact, let me, let me just try one here. So regional just type it in the void.

1106
02:30:57.900 --> 02:31:06.150
And see what that created it created a class called org Apache Spark SQL column name right. That happens to have about containers per region is the name

1107
02:31:06.570 --> 02:31:18.420
Right. So in order to do these packets of data frames because data frames are effectively based on this, this, this notion of these named columns you basically have to get the name of a column somehow. And you do that by feeding a string into dollar

1108
02:31:19.050 --> 02:31:24.750
And then you'd have to basically check if the value in the column of that name equals a certain string. And that's where you can use triple equals

1109
02:31:25.710 --> 02:31:36.000
Similarly, the other way to mess that up is, okay, let's put the dollar sign back in. So we've got our happy column on the left side, but not only to equal sides. Do we expect this to work.

1110
02:31:37.500 --> 02:31:38.520
And why what happens

1111
02:31:40.020 --> 02:31:50.970
Overloaded method value filter with alternatives. Right. And the problem here is it couldn't tell what we were trying to do right here's a couple of different functions that we might have been asking for

1112
02:31:51.390 --> 02:32:00.300
Right, we could have been asking for a filter function that operates on rose and returns these things. Or we could have asked him for that other thing, there's, there's just not enough there from what we types for to work with.

1113
02:32:00.780 --> 02:32:07.830
Right. So when you get it wrong. You have to forget the dollars to get a column name from the spring, the things that column or you forget to use the right

1114
02:32:08.160 --> 02:32:15.210
A quality operator for for figuring for matching the comparing the value of the team to call with that name, you'll get one or the other of these errors.

1115
02:32:15.390 --> 02:32:24.450
So when you see these that's why they are what they are. Right. And the second case, it's got an overlay there's more than one implementation and filter it just can't tell from what you types which one you wanted

1116
02:32:24.840 --> 02:32:30.450
And in the other case, there is no triple equals on strings triple equals is a thing on

1117
02:32:32.190 --> 02:32:36.480
Org Apache Spark sequel colony and so that that's what's happening in those

1118
02:32:37.680 --> 02:32:38.160
Make sense

1119
02:32:39.300 --> 02:32:42.690
So the thing is if you were looking at this slide, I'm sorry I haven't uploaded them yet.

1120
02:32:42.990 --> 02:32:52.350
But if you had a printed copy of this slide and you were going to circle to things like you would circle the dollar sign and you would circle the triple equals, and you would make a note that those are the things you want to remember the things you want to come to afterwards.

1121
02:32:53.220 --> 02:33:01.380
And you don't play around with it and try it. You'll, you'll get familiar with these errors because eventually you will forget the dollar sign or you will just you know he's kind of coming equal signs type

1122
02:33:03.480 --> 02:33:09.900
And so, you know, again, we can do the same sorts of stuff we did before so teleport ca goals Fairport stuff filter.

1123
02:33:11.520 --> 02:33:23.400
Okay. And so how would I get the ice of the region. Again, I would get the column name for it. I would compare it using the triple equals to whatever it's supposed to be. And I could filter. Again, if I wanted to count category, what would I do for category.

1124
02:33:26.670 --> 02:33:30.000
Category, and then two, and then to check it. I will

1125
02:33:31.650 --> 02:33:32.550
triple equals

1126
02:33:33.780 --> 02:33:37.350
teleport and we won't bother with the elevations anymore. I will just get those

1127
02:33:38.640 --> 02:33:50.730
And then of course.ca dot show you say the first five AND WE WILL THEY ARE ALL IN CALIFORNIA AND THEY THEY ARE appellate courts. So that's doing the right thing. So same operation we stopped before

1128
02:33:52.890 --> 02:34:01.380
The same sort of filter operations, same sort of semantics, but on two different days to two different representations, the day before the data sets, which contains all the case classes.

1129
02:34:01.560 --> 02:34:07.770
Now it's data frames which are these things that have named columns specific types and you know the differences didn't one of them.

1130
02:34:08.100 --> 02:34:14.490
The arguments for your maps and your filters and your folds and so on our case classes and other you know first class scholar objects.

1131
02:34:15.240 --> 02:34:22.470
And in this case, you have to generate these column names that that spark sequel knows what to do with in order to figure out what it's actually checking for

1132
02:34:24.870 --> 02:34:33.990
So, you know, you can probably start developing a sense for whether those are easy or hard to use a couple more examples that are basically the same as before.

1133
02:34:35.790 --> 02:34:45.360
Now those are basically the same. So again, take away. You know when you're using the data frames, you need to use the dollar operator to convert a string to a colony.

1134
02:34:45.750 --> 02:34:52.590
Actually, let's try it. If we mess up a colony and see what it does. This is a another thing here. So let's go back to our original thing.

1135
02:34:55.080 --> 02:34:55.500
Okay.

1136
02:34:56.520 --> 02:35:00.930
So help support CA and let's just mess. The name up and see how

1137
02:35:03.060 --> 02:35:20.580
Well, it explodes and what is the nature of the explosion analysis exception count result I so regions given input columns a whole bunch of things, not one of which is I so regions. So that's a lot better than line goes two lines split comma

1138
02:35:21.780 --> 02:35:28.710
11 or whatever. And it's just not being the right thing and you'll find out later when you get 53,000 lines of Rd and they're all crap that you didn't expect.

1139
02:35:29.400 --> 02:35:36.060
So there's a big scary stacked up on here, but the actual part of the, the actual message in the air is pretty good, right, you know,

1140
02:35:36.450 --> 02:35:43.110
It's basically telling us for some column and it just, you know, the input columns, it has to work with which it knows how this thing is created.

1141
02:35:43.440 --> 02:35:57.870
Don't got right and there's your filter. They've got so regions with USDA and you know who perhaps were pursuing a bug in the query plan or you'd actually be built the rest of this crap. But, you know, for the purposes of original music. You don't have to. So that's, that's all good.

1142
02:35:59.880 --> 02:36:05.040
Doesn't exist anywhere in the room who sees any value or data that over what we were doing. Initially, the strategies.

1143
02:36:06.600 --> 02:36:10.260
Or is it just seemed like more garbage. No, no gain at all.

1144
02:36:12.060 --> 02:36:20.850
Okay, I'm Stevens and so this gets back to stuff that we were sort of reading about earlier. So the data we've missed it, so far as coming from

1145
02:36:22.950 --> 02:36:38.850
CSV files, but the spark session on the read stuff on the spark session also has the ability to be other things, including including JSON and JSON with lines or JSON or lines JSON or whatever the hell that form is called Jason character. Character.

1146
02:36:39.930 --> 02:36:51.480
So you like in the example here, you see me read a JSON version of the report file and then we can print the schema. I think I might even have this file on this this machine. Let's get a pointer back

1147
02:36:53.130 --> 02:36:56.130
OK, so now other airport

1148
02:36:58.920 --> 02:37:01.200
Equals for JSON.

1149
02:37:03.120 --> 02:37:03.690
File.

1150
02:37:10.020 --> 02:37:11.250
So Jason, what is Jason

1151
02:37:12.870 --> 02:37:20.730
There you go. Okay, so what did we came back a data frame. It looks like a data frame that may have had some other stuff. If I now ask for other airport data.

1152
02:37:21.900 --> 02:37:22.530
Schema

1153
02:37:26.220 --> 02:37:37.410
Okay, so what do you see here it actually has some measure of schema that infer from the other airport data, it's not really compelling example because the most things, everything is a string. You can stuff.

1154
02:37:38.280 --> 02:37:45.300
And the latitude and longitude, which maybe you'd want to be incidentals and it thinks their strings, based on what it read, which is

1155
02:37:46.440 --> 02:37:47.130
You know,

1156
02:37:48.150 --> 02:37:52.920
One problem is that JavaScript itself is already pretty trouble from a system standpoint, right, like

1157
02:37:53.220 --> 02:37:58.860
There's how many notions of a quote, you know, we talked about equality of reference and equality of, like, you know, value.

1158
02:37:59.130 --> 02:38:04.350
And then look at how equality is determined to JavaScript. And the answer is often like yellow right like there's

1159
02:38:04.560 --> 02:38:12.030
There's all these JavaScript tongue twisters where like you get these simple snippets of code that don't do anything that you expect or anything that's understandable because it was

1160
02:38:12.480 --> 02:38:27.420
JavaScript has like two good ideas in it, and a whole bunch of stuff that was done by like one drunk over a weekend before he started funding creepy California valid initiatives to discriminate against people, seven years later and So JavaScript is it it's

1161
02:38:29.130 --> 02:38:39.750
Going back to my technology taxonomy right where you can basically break things into three categories. There's a useful widely used and actually good.

1162
02:38:40.200 --> 02:38:48.420
And JavaScript is only really, I think, in one of those categories, it's divided us right and you know in verges on useful on a good day. And then the rest of

1163
02:38:48.630 --> 02:38:56.010
The system like whereas lots of other stuff is is useful and widely used, but not good, and something that's all three. That's a rare and precious gem.

1164
02:38:57.300 --> 02:39:05.610
But anyway, so we've now pull this data frame in front of a JSON file and then we can do a joint. So joining the airport data. And let me make sure I actually have

1165
02:39:06.750 --> 02:39:14.700
To work with equal airports joy. Other airport airport data.

1166
02:39:16.080 --> 02:39:16.710
Yeah. OK.

1167
02:39:17.790 --> 02:39:19.020
Comma airports.

1168
02:39:21.270 --> 02:39:22.110
Name.

1169
02:39:24.540 --> 02:39:33.240
Equals other airport data name like these prisons wrong probably select

1170
02:39:35.100 --> 02:39:35.730
Dollars.

1171
02:39:36.810 --> 02:39:38.340
Comma airports.

1172
02:39:39.750 --> 02:39:43.710
I'm starting to really feel like I Walked the plank here and this is never going to work. But let's find out.

1173
02:39:50.250 --> 02:39:52.440
Let's see what's in it show five

1174
02:39:54.780 --> 02:39:57.180
Oh miracle of miracles. It kind of works right so

1175
02:39:58.410 --> 02:40:10.140
We have that one airports data frame that we had, we had this new one that we got from the other from JSON file. So remember we got one of them from like loading CSV files and converting it data frames that nice table with columns.

1176
02:40:12.120 --> 02:40:17.070
Loading script garbage and we got games types columns and then you did come to the new style joy.

1177
02:40:18.120 --> 02:40:31.200
Take airports join it with other airport needed by finding the records were airports name equals other airport needed me and then just project out just these calls I get airport. Same time zone, but I guess that's

1178
02:40:32.370 --> 02:40:40.260
A no makeup. Right. Yeah. Oh, so you're skeptical. Yeah, yeah. You want your script. It seems like fraud.

1179
02:40:43.020 --> 02:40:44.100
Yeah, come here.

1180
02:40:53.880 --> 02:41:07.980
Yeah. So what what's happening here. So you get to decode that you have to look at the job does the documentation for spark sequel. And what you see is what the type signature of joints. So join on a data frame wants another data frame.

1181
02:41:08.490 --> 02:41:14.760
And then basically this thing here. You're going to actually be explicitly. These are going to return things

1182
02:41:15.510 --> 02:41:25.440
There's basically applying that that on these things that takes a name and effectively returns a colony. So what I think this is going to be at my second try it airports.

1183
02:41:26.400 --> 02:41:34.710
Let's give it up in the in the void here just even just returns airports and that's an order patchy sequel column.

1184
02:41:35.100 --> 02:41:42.690
Right, so the apply method on data frame. Given a string will basically give you back the column name for that string right

1185
02:41:43.230 --> 02:41:53.160
So effectively is doing the same thing the dollar sign would have right but um you know it's here. We're making it clear when we're joining where these tables are coming from. Right.

1186
02:41:58.350 --> 02:42:01.350
Could you slap dollar signs in there, you know,

1187
02:42:02.640 --> 02:42:16.740
Oh yeah. But that's, yeah, that's the other part of the thing. The question is, what it works here. Right. Right. Yeah. And the answer here is it might because from the positions. It knows which one of these is which. But let's find out. Oh, it did not like

1188
02:42:18.420 --> 02:42:21.090
Oh, yeah. So there's the problem, right, the

1189
02:42:22.590 --> 02:42:28.200
The column name returned by the airport's method actually kept track of which data frame that came from.

1190
02:42:33.240 --> 02:42:34.140
Us. We're sorry

1191
02:42:43.140 --> 02:42:48.420
Oh yeah, so see so big. So there's a problem, right, is that when I when I took the name of the data frame.

1192
02:42:49.110 --> 02:42:58.650
perrins IE, it's calling it apply nested on data frame and then the name of column I got back a an object that contains not only the name of the column boat which data frame it belongs to

1193
02:42:59.010 --> 02:43:09.750
Right, which is enough to tell them apart when I stripped them off and just created these now name is ambiguous right it's a trivial. The true benefit because as far as it knows. Like, what does it even comparing right but what I really want is

1194
02:43:10.020 --> 02:43:24.450
I want to join on the name column from one data frame with the name column from the other database. And if I go back to just the dollar things I end up creating a column thing doesn't have enough information to tell apart. It's easy to win us, which is a name that Jesus is coming

1195
02:43:25.650 --> 02:43:33.300
From CSP will escape or your or your constructing a predicate that so trivial. There's no other possible I'd be doing

1196
02:43:33.630 --> 02:43:40.920
Right. And in the cases with filter you know filter is only operating on that one data frame right if I say dollar name. What could I, what else could I possibly be talking about right.

1197
02:43:41.190 --> 02:43:44.310
There's only one real data frame and the university replaying and at that point.

1198
02:43:44.700 --> 02:43:53.430
So when I do dollar name and it creates one of these org Apache sequel spark. Who would win column name things. It's sort of knows what to do. The problem here is, and the error message actually pretty good at.

1199
02:43:54.150 --> 02:44:07.560
Constructing truly true predicate equals name equals name, perhaps you need to use this right. You know, I've already got that trick up and then when it actually gets a little further and tries to actually work these things out. It's just lost, right, because it has no idea which one for

1200
02:44:09.060 --> 02:44:11.070
The other right so you please

1201
02:44:13.140 --> 02:44:19.080
repeat again for me. So you mean this guy here, right, to get to the joy. What is my

1202
02:44:20.580 --> 02:44:20.790
Pretty

1203
02:44:23.460 --> 02:44:28.140
Well, by the time you're down to select the joining is done right now, you've just got one result in a different

1204
02:44:28.380 --> 02:44:39.510
The join results in a new data frame. So when I say dollar I didn't right there's only one identity in the universe anymore, right, what's what's happened here is that we started with. So airports.

1205
02:44:40.800 --> 02:44:44.910
Airports join takes another data frame as well as a predicate to define what the joint.

1206
02:44:45.330 --> 02:44:53.220
Right, and then we're using these these access or these the supply methods on air upon the data frames in order to actually get on ambiguous columns.

1207
02:44:53.490 --> 02:44:58.710
Right. And then once that's done, we get a data, we get a data frame that just contains the roads that are in the join

1208
02:44:58.980 --> 02:45:06.390
Right, and then select is basically just project. The album filtering all but a couple columns back we're going to throw away all the columns, except the ones we didn't Select All right.

1209
02:45:06.660 --> 02:45:17.910
And so at that point we can go back to using dollar because, you know, we're just in the same data frame. There's no ambiguity. And then we had in the things that you had to say specifically for the end because then we can move on.

1210
02:45:20.010 --> 02:45:23.160
In fact, yeah. In fact, if we roll this back

1211
02:45:24.630 --> 02:45:25.770
Let's just, let's try this.

1212
02:45:30.450 --> 02:45:41.520
And we have the same problem as you'd expect. Right. And again, reference name is ambiguous could be a name. And the problem is they didn't join. We have named columns from both right. Yeah.

1213
02:45:45.660 --> 02:45:55.170
Yeah, airports is our original one that we created from the CSV file. Other airport a data frame, if I be replaced the dataset with the data frame when we moved over to the difference, but at the same thing.

1214
02:45:59.610 --> 02:46:02.730
So the question is, what is the type of airports, right.

1215
02:46:05.040 --> 02:46:06.510
Oh, it is still there. Yep. Okay.

1216
02:46:08.160 --> 02:46:09.210
Yep, yep.

1217
02:46:14.520 --> 02:46:14.730
Yep.

1218
02:46:17.640 --> 02:46:17.880
Yep.

1219
02:46:19.590 --> 02:46:26.670
Yeah yeah and and that's actually another interesting about the different data sets, is that there's lots of these overlap cases where things are sort of course

1220
02:46:27.000 --> 02:46:37.470
So if part of your project is being done in one and part of it is doing in the upper you may not have very much of a journey to go to get them to actually nicely work together, or in some cases not at all like they'll just happened accidentally work.

1221
02:46:48.210 --> 02:46:55.350
Yeah because data frames have column names of datasets don't either datasets don't really have columns right there. Just a sequence of instances of this class.

1222
02:46:56.370 --> 02:46:58.320
Right. But, you know, in the

1223
02:46:59.910 --> 02:47:08.820
You know, in the cases where you can, you know, and the issue there is when you call that apply method on one of the other. What does it come back with right and you know in

1224
02:47:09.270 --> 02:47:15.000
A bunch of the cases the API's turned back they returned back. Basically the only thing that can really be useful or makes sense and

1225
02:47:15.300 --> 02:47:26.850
You know, it's not really a coincidence. It's by design. It happens to work out everything that could comfortably consumer. Does that make sense. Yeah, yeah. So that, so those are actually really, those are those are great questions like their

1226
02:47:28.050 --> 02:47:35.070
Pitch those details and, you know, that's how when you see these things happen. You can kind of explore right like thinking about level. If I take $1 sign off, what does it do

1227
02:47:36.150 --> 02:47:45.360
And all the time. Like, you know, the message is look scary because there's all these stacked up trash, but you know it's 2018 if you're stuck Trump is less than 4700 lines just don't feel like you've done any work today.

1228
02:47:45.780 --> 02:47:51.660
Um, but, you know, most startups to have the 4700 lines of garbage that come out. There's only two or 123 of them.

1229
02:47:52.470 --> 02:47:59.910
And when you're lucky, they're at one of the ends. And in this case, there's only a couple lines to three lines that are in there right at the very first part that you see.

1230
02:48:00.360 --> 02:48:08.640
You know, that's the best kind of stack traces regardless of blank ones where the interesting thing is obvious because it's small and it's one of the ends. If you're using one of these

1231
02:48:09.000 --> 02:48:18.360
Spraying spraying or one of these dependency injection frameworks for everything was wired up by some extra mystic mechanism that we've actually behavior is specified nowhere, other than what it just happened to have done.

1232
02:48:18.690 --> 02:48:24.690
And you get these gigantic staff friends and the probably somewhere in the middle right you know 800 frames, and it seemed like

1233
02:48:25.350 --> 02:48:35.340
396 to 399 right and unless you really know the details of that code base infinitely as well as all the library crap that wired together. This stuff is hard to debug right

1234
02:48:35.640 --> 02:48:42.450
Like it's just all this like submerged in the trash complexity that, you know, I guess it makes people feel enterprises, but I don't know.

1235
02:48:44.220 --> 02:48:44.670
I mean,

1236
02:48:47.580 --> 02:48:52.830
Oh, so what do you want to know the time. Yes. So when you type the name of the thing and the spark shell.

1237
02:48:53.280 --> 02:49:05.370
Basically, the result will come out with whatever it's to stream that introduces so this stuff on the right is the result of the to say that she realized that as long. So we switched it down without the dots and stuff on the left is the type, you know, just like if I did.

1238
02:49:06.360 --> 02:49:21.960
val x equals 137 and then if I later asked her ex, it would say, hey, x isn't like so. And you know, when you are working with stuff in Scala. That's one nice thing is it's always it's generally easy modulo certain tricky quarters or ratio, it's, it's fairly easy to ask

1239
02:49:23.400 --> 02:49:32.310
And in the shower. It's usually going to give you an answer. The places where you really see a ratio problems is where you have complex parametric polymorphous and then generics and lots of hyper parameters or

1240
02:49:32.670 --> 02:49:36.270
You know where you really get want to take the runtime that the thing discarded long ago.

1241
02:49:36.870 --> 02:49:43.470
But, you know, in these environments that cannot happen that much because typically sparks equal in particular it's often used.

1242
02:49:43.770 --> 02:49:55.020
Reflection well constructing these things to figure out what the types are then remembered what the types were and then actually kept names right so you know it's almost it's almost redefine the typewriter anyway.

1243
02:49:57.240 --> 02:50:02.760
Did that phrase redefine typewriters. Does that make sense to everybody. Does that mean something or is that just like we're made up.

1244
02:50:04.170 --> 02:50:05.970
To whom are those weird made up rumors.

1245
02:50:08.460 --> 02:50:12.030
Okay, so people are happy generally with that expression or not happy.

1246
02:50:13.170 --> 02:50:16.920
People are making facial expressions. But I don't know how to interpret them.

1247
02:50:18.180 --> 02:50:19.110
Yeah okay so

1248
02:50:20.430 --> 02:50:25.110
Parameters classes. Prepare metric polymorphous and we're so called generics in Java. Yeah.

1249
02:50:30.060 --> 02:50:39.030
No, no. Yeah, we're gonna stop racing. Anyway, so, and it's recorded anyway so you can get the last seven minutes of whatever I deteriorating to before the conference. Oh.

1250
02:50:41.490 --> 02:50:42.840
Yes, you need to catch a bus, you don't have

1251
02:50:46.470 --> 02:50:47.250
Fun.

1252
02:50:50.310 --> 02:50:56.310
Um, yeah. So if you haven't Java a list of integer or a list of string.

1253
02:50:57.690 --> 02:51:03.300
You know, you define that time by saying lists angle brackets string list equals new list, do whatever you want with it.

1254
02:51:03.720 --> 02:51:09.090
But by the time that compiled code gets down to the Java virtual machine. It's forgotten about stream. It's just replaced string object.

1255
02:51:09.870 --> 02:51:15.930
Right. And so when you're at the front end and the ID or when you're in the compiler. It's still a note about stream.

1256
02:51:16.290 --> 02:51:21.270
But in the final stages it peels off the strings and just, you know, it could be streaming. It could be, it could be

1257
02:51:21.870 --> 02:51:27.090
Airport, whatever, it's going to boil off and discard those and just produce collection of objects that the underlying thing.

1258
02:51:27.450 --> 02:51:34.710
And so there are things that you can know and check it in the ID or at compile time or during the compile that afterwards you will know

1259
02:51:35.670 --> 02:51:48.090
Because the type parameters been taken away in this process called ratio Java isn't the only language does it smell it does it in order to be basically compatible with way Java does it on the JVM C sharp used to do it, sort of, but doesn't know.

1260
02:51:48.750 --> 02:51:58.740
Haskell does it and it does have this one problem that there are certain cases where you might actually want the type information on the thing, but it's just no longer exists. So you're a bit stuck.

1261
02:51:59.610 --> 02:52:09.960
In spark because of all the extra reflection and typing and memorizing these types and keeping track of them that it does it sort of compensates for that monster in the in the collections that uses

1262
02:52:10.410 --> 02:52:16.860
But when you are writing running naked seller Java, you will occasionally when these things and then the ID will underline it and say, you know, uncheck to

1263
02:52:17.220 --> 02:52:25.740
You know exception if not exception, but warning unchecked expression or unchecked cast or something. Because it just doesn't know enough at that point to know if the thing can actually be converted

1264
02:52:26.730 --> 02:52:36.300
And this was done for backwards compatibility and there was there was furious debate over a 10 people still argue about it 2015 years later, whenever

1265
02:52:38.340 --> 02:52:59.370
I see them every summer. It's great. Um, but, okay, so, um, yeah. So one last quick thing to throw in. And then when we're not stopping point for tonight is views and sequel queries. So what are these there one sort of last cherry on the sparks equal bananas or cherries m&s put

1266
02:53:00.450 --> 02:53:16.500
Some kind of dessert cherries on it, whatever. So anyway, um, so if we've got airports def WHICH WE STILL HAVE WE SO WE CAN DO airports def create or replace view view MP

1267
02:53:17.700 --> 02:53:18.000
MP

1268
02:53:19.140 --> 02:53:20.130
Called airports.

1269
02:53:21.840 --> 02:53:22.290
Okay.

1270
02:53:23.490 --> 02:53:28.920
And then we can also take our other airports data create for replaced him with you.

1271
02:53:30.180 --> 02:53:35.520
Have global attempt to explain what the hell else is competing and let's call that one other airports.

1272
02:53:38.400 --> 02:53:41.580
Okay, so now we have these views. So now we can use Spark.

1273
02:53:42.390 --> 02:53:50.820
SQL in kind of this old school way that you know maybe it's sometimes handy, but like you probably don't want right programs that are going to be for the ages this way because it's obviously fragile.

1274
02:53:51.270 --> 02:54:04.200
But we can basically pass a string that contains a sequel query. Select star from airports where ISO region equals us ca close these things show

1275
02:54:06.330 --> 02:54:14.430
Okay, and then some spark stuff happens. Maybe, and we get results. So what did we do there, right. We just basically

1276
02:54:15.060 --> 02:54:21.900
We call the sequel method on the spark session after a string represents a literal sequel query.

1277
02:54:22.170 --> 02:54:30.480
And because we create these views which had. Hey, take that data frame and create a sequel view, which means when you do one of the sequel expressions written as a string like this.

1278
02:54:30.630 --> 02:54:34.740
Pretend there actually is a database table called whatever that thing is for it to work against

1279
02:54:35.040 --> 02:54:49.260
So if you were wanting to write sequel queries or you had a bunch that come from somewhere that you just had static Lee or whatever you can create a replace these views of the data frame and then apply that against them using that name as the as the name of the deal with

1280
02:54:50.610 --> 02:54:51.000
These

1281
02:54:53.730 --> 02:54:57.030
Yeah there yet another weird in memory like goblin

1282
02:54:59.070 --> 02:55:11.610
Yeah, like when you when you exit sparkle and come back they no longer exist, they're there, they're throwing away because they were associated with your, with your session and you know they didn't for that is that you declared them using this method on

1283
02:55:13.830 --> 02:55:22.980
That actually that leads the question completely open up but yeah yeah and so and so it's hard to get to allow you to do this sort of cans type down, you know,

1284
02:55:23.790 --> 02:55:28.860
English equally type query thing which you know for certain things. Maybe it's easier than typing. The other thing.

1285
02:55:29.130 --> 02:55:38.130
On the other hand, like nothing is going to complete any of those names in that string that nothing is going to verify. There really is a field called ISO underscore region or anything like that.

1286
02:55:38.340 --> 02:55:41.970
So you already saw like a couple of the other APIs we saw earlier tonight.

1287
02:55:42.240 --> 02:55:51.630
In some sense, they're almost a little bit more safe, even though you don't have like your traditional sequel query. It's sort of like broken into pieces and parts of your sort of reorder and it happens in stages with dots between them.

1288
02:55:51.900 --> 02:56:01.050
But at least within each of the stages, you can hit Tab, like a monkey something familiar comes up after you type CEO you return country code, whatever it is.

1289
02:56:01.500 --> 02:56:14.160
Whatever it is, you know, you just type something in a bunch of times and sort of code by Braille, whereas in the raw sequel query thing in there. You can write. It's just a string. And if you get something wrong, it's going to do about what you expect airports.

1290
02:56:15.180 --> 02:56:15.780
Where

1291
02:56:17.520 --> 02:56:24.000
You know, and, well, you know, horrible stack trace, but at least stuff is writing your table or view not found airports as

1292
02:56:24.240 --> 02:56:33.450
Well, okay, that's a pretty good error message. It's only one light of Aramis legitimacy actually in the factory. So it's already met by lowered expectations for what constitutes your local usable cemeteries.

1293
02:56:35.250 --> 02:56:40.320
So I don't know if you use these a lot i don't know that i don't think i used to

1294
02:56:41.730 --> 02:56:49.470
I don't know that. I know a lot of people use them a lot. But knowing it's there, right. If you happen to stumble across it. Or if you're in a spot where it just happens to be can be at least an option.

1295
02:56:50.310 --> 02:56:59.370
But, you know, it obviously has a lot of disadvantages compared to the things you just did before, right, which is that, you know, I can type nonsense like that. And I don't really have any hint until it's really too late.

1296
02:57:00.750 --> 02:57:04.980
You can always fix it. But, you know, still it is, but it doesn't mean that

1297
02:57:06.900 --> 02:57:07.350
It's there.

1298
02:57:09.150 --> 02:57:09.930
It doesn't excite me

1299
02:57:11.310 --> 02:57:15.390
It doesn't excite me that much, you know, perhaps you have a feeling using different motivations.

1300
02:57:16.410 --> 02:57:18.600
You view these things differently. But, you know,

1301
02:57:20.640 --> 02:57:20.910
My

1302
02:57:22.950 --> 02:57:29.160
Individual results may vary. There's lots of disclaimers on stuff. And now we're coming up on nine o'clock, so we should probably stop.

1303
02:57:29.400 --> 02:57:35.640
But on the upside. We're basically done with tonight's material. There's just a tiny little bit of stuff and it's mostly just like

1304
02:57:36.030 --> 02:57:42.270
Things you want to know the name of so that if they ever across your desk, you can, you know, go look up the details and use them.

1305
02:57:42.690 --> 02:57:54.450
And then we'll be able to finish off spark material next week and I'll have a little bit on on getting a spark project set up running locally. So that testing and have nice completion on my docs and stuff like that that way.

1306
02:57:58.050 --> 02:58:01.380
Um, do you have intelligent idea. I haven't thought

1307
02:58:04.860 --> 02:58:09.690
Oh, maybe this isn't going to be as easy as I thought it would be good thing. I have a whole

1308
02:58:11.730 --> 02:58:16.320
Eclipse. Okay. Um, I'll probably try to build it as a Maven project MAVEN is

1309
02:58:16.710 --> 02:58:22.260
We can talk about maybe next week. But, I mean, it sucks to but um but the nice thing is that

1310
02:58:22.500 --> 02:58:32.850
You know, at least maybe it's the same everywhere. And so intelligent and smart and Eclipse will both be able to consumer Maven project and like, bring it up from the just the textual description of the project and then you'll basically have

1311
02:58:33.180 --> 02:58:42.240
The clips are intelligent as you are comfortable with to work on. So almost ended up as an eclipse as a Maven thing. So it's independent to be at ease, and they can be called Indians.

1312
02:58:43.470 --> 02:58:48.120
At all, I'll probably get it up and posted on canvas actually even before next week if people want to play with it.

1313
02:58:49.710 --> 02:58:51.960
By just didn't get a chance to today because of reasons.

1314
02:58:53.970 --> 02:58:59.910
Okay, so in our closing Halloween moment said, Does anybody in class or online have questions.

1315
02:59:08.670 --> 02:59:14.220
Okay, I guess, stop recording and I can begin with the whatever happens after the recording stops.

